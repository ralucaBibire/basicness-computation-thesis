{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Calcolo basicness score per tutti i synset di WN\n",
                "Uno score alto indica che il synset è più basic o generico."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.corpus import wordnet\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
                "from sklearn.metrics import root_mean_squared_error, r2_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = MinMaxScaler()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "only_synset_with_agreement = False\n",
                "select_top_n_features = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataframe con le features già calcolate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dimensione df_merged_features: 82115\n",
                        "Colonne df_merged_features: ['Synset', 'Depth', 'Normalized Depth', 'Agreement score', 'isHard mean', 'timeDiffs mean', 'Normalized timeDiffs mean', 'LCH Similarity', 'Normalized LCH Similarity', 'Frequency', 'Normalized Frequency', 'Synonymy', 'Normalized Synonymy', 'In CBT']\n"
                    ]
                }
            ],
            "source": [
                "df_features_file = 'features/df_merged_features.csv'\n",
                "df_merged_features = pd.read_csv(df_features_file)\n",
                "# print(f\"{df_merged_features[:10]}\")\n",
                "print(f\"Dimensione df_merged_features: {len(df_merged_features)}\")\n",
                "print(f\"Colonne df_merged_features: {df_merged_features.columns.tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dimensione df_merged_features_clean: 504\n"
                    ]
                }
            ],
            "source": [
                "# Rimuovo dal df tutti i synset che non sono nel gold standard\n",
                "df_merged_features_clean = df_merged_features.copy()\n",
                "df_merged_features_clean = df_merged_features_clean.dropna(subset=['Agreement score'])\n",
                "print(f\"Dimensione df_merged_features_clean: {len(df_merged_features_clean)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"Synset('whole.n.02')\": 1.0, \"Synset('motivation.n.01')\": 0.9, \"Synset('discovery.n.01')\": 0.9, \"Synset('sleeper.n.09')\": 0.5, \"Synset('hit.n.03')\": 1.0, \"Synset('shooting.n.01')\": 0.7, \"Synset('enfilade.n.01')\": 0.0, \"Synset('designation.n.03')\": 0.0, \"Synset('manipulation.n.01')\": 0.2, \"Synset('mind_game.n.02')\": 0.2, \"Synset('decision.n.01')\": 1.0, \"Synset('ruse.n.01')\": 0.0, \"Synset('means.n.01')\": 1.0, \"Synset('royal_road.n.01')\": 0.3, \"Synset('step.n.03')\": 1.0, \"Synset('trip.n.06')\": 1.0, \"Synset('motion.n.03')\": 0.8, \"Synset('inversion.n.08')\": 0.7, \"Synset('activity.n.01')\": 1.0, \"Synset('habit.n.02')\": 0.9, \"Synset('second_nature.n.01')\": 0.3, \"Synset('dance_step.n.01')\": 0.4, \"Synset('glissade.n.01')\": 0.0, \"Synset('position.n.06')\": 1.0, \"Synset('associateship.n.01')\": 0.0, \"Synset('care.n.01')\": 1.0, \"Synset('myringotomy.n.01')\": 0.0, \"Synset('desensitization_technique.n.01')\": 0.0, \"Synset('function.n.03')\": 0.9, \"Synset('behalf.n.01')\": 0.0, \"Synset('sea-duty.n.01')\": 0.0, \"Synset('attempt.n.01')\": 0.8, \"Synset('contribution.n.01')\": 0.3, \"Synset('end.n.11')\": 1.0, \"Synset('striving.n.01')\": 0.0, \"Synset('test.n.05')\": 1.0, \"Synset('double_blind.n.01')\": 0.0, \"Synset('clasp.n.02')\": 0.0, \"Synset('hammerlock.n.01')\": 0.0, \"Synset('reaction.n.03')\": 0.9, \"Synset('meteortropism.n.01')\": 0.0, \"Synset('look.n.02')\": 1.0, \"Synset('eye-beaming.n.01')\": 0.0, \"Synset('education.n.01')\": 0.9, \"Synset('course.n.01')\": 1.0, \"Synset('art_class.n.01')\": 0.5, \"Synset('workshop.n.02')\": 0.5, \"Synset('arboriculture.n.01')\": 0.0, \"Synset('use.n.01')\": 1.0, \"Synset('play.n.06')\": 1.0, \"Synset('exploitation.n.01')\": 0.2, \"Synset('unitization.n.04')\": 0.0, \"Synset('military_action.n.01')\": 0.4, \"Synset('armageddon.n.02')\": 0.2, \"Synset('violence.n.01')\": 1.0, \"Synset('war.n.01')\": 1.0, \"Synset('fire.n.02')\": 0.9, \"Synset('counterbombardment.n.01')\": 0.0, \"Synset('jihad.n.01')\": 0.3, \"Synset('procedure.n.01')\": 0.8, \"Synset('rest.n.02')\": 1.0, \"Synset('caravanning.n.01')\": 0.0, \"Synset('commercial_enterprise.n.02')\": 0.3, \"Synset('deal.n.01')\": 0.9, \"Synset('penny_ante.n.01')\": 0.0, \"Synset('management.n.01')\": 0.9, \"Synset('toleration.n.02')\": 0.1, \"Synset('road_rage.n.01')\": 0.0, \"Synset('fight.n.02')\": 1.0, \"Synset('affray.n.02')\": 0.1, \"Synset('aid.n.02')\": 0.8, \"Synset('auspices.n.01')\": 0.0, \"Synset('crest.n.05')\": 0.1, \"Synset('chine.n.02')\": 0.1, \"Synset('homo.n.02')\": 0.7, \"Synset('homo_sapiens.n.01')\": 0.4, \"Synset('avenue.n.02')\": 0.4, \"Synset('back.n.08')\": 1.0, \"Synset('base.n.08')\": 0.9, \"Synset('bell_gable.n.01')\": 0.0, \"Synset('bilge.n.02')\": 0.0, \"Synset('book.n.02')\": 1.0, \"Synset('boucle.n.01')\": 0.0, \"Synset('brass_monkey.n.01')\": 0.0, \"Synset('caddy.n.01')\": 0.2, \"Synset('can.n.01')\": 1.0, \"Synset('cantle.n.01')\": 0.0, \"Synset('car.n.01')\": 1.0, \"Synset('cocktail_lounge.n.01')\": 0.0, \"Synset('conservatory.n.02')\": 0.4, \"Synset('cusp.n.01')\": 0.0, \"Synset('dwelling.n.01')\": 0.0, \"Synset('fabric.n.01')\": 0.5, \"Synset('fresco.n.01')\": 0.4, \"Synset('front.n.04')\": 0.9, \"Synset('handle.n.01')\": 0.8, \"Synset('hermitage.n.01')\": 0.2, \"Synset('hose.n.03')\": 0.3, \"Synset('ladies'_room.n.01')\": 0.5, \"Synset('lighter.n.02')\": 0.8, \"Synset('man.n.10')\": 1.0, \"Synset('model_t.n.01')\": 0.0, \"Synset('nose_cone.n.01')\": 0.0, \"Synset('oeuvre.n.01')\": 0.1, \"Synset('painting.n.01')\": 1.0, \"Synset('palestra.n.01')\": 0.4, \"Synset('picture.n.01')\": 1.0, \"Synset('pommel.n.01')\": 0.0, \"Synset('radiator_cap.n.01')\": 0.0, \"Synset('recording_studio.n.01')\": 0.2, \"Synset('room.n.01')\": 1.0, \"Synset('safety_match.n.01')\": 0.2, \"Synset('sawhorse.n.01')\": 0.0, \"Synset('school.n.02')\": 1.0, \"Synset('side_yard.n.01')\": 0.1, \"Synset('stopcock.n.01')\": 0.0, \"Synset('street.n.01')\": 1.0, \"Synset('thing.n.04')\": 1.0, \"Synset('toilet.n.01')\": 1.0, \"Synset('top.n.09')\": 1.0, \"Synset('trestle.n.02')\": 0.0, \"Synset('viewgraph.n.01')\": 0.0, \"Synset('wall.n.01')\": 1.0, \"Synset('way.n.06')\": 1.0, \"Synset('white.n.11')\": 1.0, \"Synset('workplace.n.01')\": 0.6, \"Synset('activeness.n.02')\": 0.0, \"Synset('dynamism.n.03')\": 0.2, \"Synset('air.n.03')\": 1.0, \"Synset('vibration.n.04')\": 0.4, \"Synset('looseness.n.05')\": 0.1, \"Synset('wiggliness.n.01')\": 0.0, \"Synset('determination.n.02')\": 0.6, \"Synset('doggedness.n.01')\": 0.0, \"Synset('sanctimoniousness.n.01')\": 0.0, \"Synset('sound.n.01')\": 1.0, \"Synset('voice.n.01')\": 1.0, \"Synset('luminosity.n.01')\": 0.5, \"Synset('incandescence.n.02')\": 0.0, \"Synset('pace.n.03')\": 0.7, \"Synset('promptness.n.01')\": 0.0, \"Synset('shape.n.01')\": 1.0, \"Synset('oblateness.n.01')\": 0.0, \"Synset('probability.n.01')\": 0.8, \"Synset('cross_section.n.03')\": 0.1, \"Synset('amount.n.02')\": 0.9, \"Synset('quantity.n.02')\": 1.0, \"Synset('number.n.01')\": 1.0, \"Synset('multitudinousness.n.01')\": 0.0, \"Synset('monetary_value.n.01')\": 0.2, \"Synset('assessment.n.03')\": 0.4, \"Synset('interest.n.03')\": 1.0, \"Synset('shrillness.n.01')\": 0.0, \"Synset('human_body.n.01')\": 0.8, \"Synset('person.n.02')\": 1.0, \"Synset('eye.n.01')\": 1.0, \"Synset('peeper.n.02')\": 0.0, \"Synset('process.n.05')\": 0.9, \"Synset('spinal_column.n.01')\": 0.2, \"Synset('face.n.01')\": 1.0, \"Synset('countenance.n.03')\": 0.0, \"Synset('ability.n.02')\": 1.0, \"Synset('puppetry.n.01')\": 0.0, \"Synset('art.n.03')\": 1.0, \"Synset('bonding.n.02')\": 0.2, \"Synset('convention.n.02')\": 0.4, \"Synset('protocol.n.03')\": 0.5, \"Synset('attention.n.01')\": 1.0, \"Synset('advertence.n.01')\": 0.2, \"Synset('feeling.n.06')\": 1.0, \"Synset('sprachgefuhl.n.01')\": 0.0, \"Synset('touch.n.10')\": 1.0, \"Synset('cutaneous_sensation.n.01')\": 0.0, \"Synset('line.n.29')\": 1.0, \"Synset('rubicon.n.02')\": 0.0, \"Synset('thinking.n.01')\": 0.9, \"Synset('basis.n.02')\": 0.9, \"Synset('meat_and_potatoes.n.01')\": 0.6, \"Synset('nature_study.n.01')\": 0.2, \"Synset('estimate.n.01')\": 0.6, \"Synset('guess.n.02')\": 0.9, \"Synset('real_world.n.01')\": 0.8, \"Synset('reading.n.03')\": 1.0, \"Synset('fact.n.01')\": 1.0, \"Synset('detail.n.01')\": 1.0, \"Synset('particular.n.01')\": 1.0, \"Synset('nook_and_cranny.n.01')\": 0.0, \"Synset('circumstance.n.03')\": 0.5, \"Synset('justification.n.01')\": 0.4, \"Synset('source.n.03')\": 0.9, \"Synset('muse.n.02')\": 0.3, \"Synset('judgment.n.01')\": 0.9, \"Synset('predetermination.n.02')\": 0.1, \"Synset('kind.n.01')\": 1.0, \"Synset('stripe.n.04')\": 0.4, \"Synset('rule.n.11')\": 1.0, \"Synset('lateral_thinking.n.01')\": 0.0, \"Synset('quality.n.03')\": 0.9, \"Synset('texture.n.02')\": 0.8, \"Synset('value.n.01')\": 1.0, \"Synset('plot_element.n.01')\": 0.0, \"Synset('point.n.01')\": 1.0, \"Synset('strange_attractor.n.01')\": 0.0, \"Synset('part.n.09')\": 1.0, \"Synset('hypothesis.n.02')\": 0.3, \"Synset('plot.n.01')\": 1.0, \"Synset('intrigue.n.01')\": 0.0, \"Synset('principle.n.01')\": 0.7, \"Synset('pillar_of_islam.n.01')\": 0.1, \"Synset('meaning.n.02')\": 1.0, \"Synset('kernel.n.03')\": 0.0, \"Synset('stuff.n.07')\": 1.0, \"Synset('undertone.n.02')\": 0.2, \"Synset('exemplar.n.01')\": 0.4, \"Synset('character.n.04')\": 0.9, \"Synset('soubrette.n.02')\": 0.0, \"Synset('form.n.03')\": 1.0, \"Synset('strand.n.01')\": 0.0, \"Synset('prodigy.n.03')\": 0.1, \"Synset('promise.n.02')\": 0.8, \"Synset('rainbow.n.02')\": 0.9, \"Synset('thought.n.03')\": 1.0, \"Synset('mainstream.n.01')\": 0.5, \"Synset('experience.n.02')\": 1.0, \"Synset('respect.n.03')\": 1.0, \"Synset('reputation.n.03')\": 0.6, \"Synset('newspaper.n.01')\": 1.0, \"Synset('gazette.n.01')\": 0.1, \"Synset('word.n.01')\": 1.0, \"Synset('demonstrative_pronoun.n.01')\": 0.1, \"Synset('heading.n.01')\": 0.3, \"Synset('title.n.01')\": 1.0, \"Synset('argument.n.05')\": 0.9, \"Synset('writing.n.03')\": 1.0, \"Synset('margin.n.05')\": 0.7, \"Synset('space.n.07')\": 1.0, \"Synset('conclusion.n.08')\": 0.8, \"Synset('peroration.n.02')\": 0.0, \"Synset('handwriting.n.01')\": 0.6, \"Synset('calligraphy.n.01')\": 0.1, \"Synset('jotter.n.01')\": 0.0, \"Synset('card.n.08')\": 1.0, \"Synset('opinion.n.04')\": 1.0, \"Synset('plea_bargain.n.01')\": 0.0, \"Synset('instruction.n.04')\": 0.9, \"Synset('link.n.06')\": 1.0, \"Synset('issue.n.02')\": 0.9, \"Synset('edition.n.03')\": 0.9, \"Synset('message.n.02')\": 1.0, \"Synset('movie.n.01')\": 1.0, \"Synset('scene.n.04')\": 1.0, \"Synset('outtake.n.01')\": 0.1, \"Synset('spaghetti_western.n.01')\": 0.0, \"Synset('record.n.05')\": 1.0, \"Synset('news.n.01')\": 1.0, \"Synset('latest.n.01')\": 0.9, \"Synset('sign.n.01')\": 1.0, \"Synset('token.n.02')\": 0.6, \"Synset('higher_law.n.01')\": 0.1, \"Synset('impudence.n.01')\": 0.2, \"Synset('answer.n.01')\": 1.0, \"Synset('rescript.n.01')\": 0.1, \"Synset('obiter_dictum.n.02')\": 0.0, \"Synset('fun.n.02')\": 1.0, \"Synset('jocosity.n.01')\": 0.0, \"Synset('direction.n.06')\": 0.9, \"Synset('numeral.n.01')\": 0.6, \"Synset('antilogarithm.n.01')\": 0.0, \"Synset('act.n.04')\": 1.0, \"Synset('show-stopper.n.01')\": 0.0, \"Synset('artwork.n.01')\": 0.7, \"Synset('illustration.n.01')\": 0.4, \"Synset('music.n.01')\": 1.0, \"Synset('theme.n.03')\": 0.8, \"Synset('diminution.n.02')\": 0.1, \"Synset('secondo.n.01')\": 0.2, \"Synset('accompaniment.n.02')\": 0.1, \"Synset('descant.n.01')\": 0.1, \"Synset('meter.n.03')\": 0.9, \"Synset('anapest.n.01')\": 0.0, \"Synset('speech.n.02')\": 1.0, \"Synset('cry.n.01')\": 1.0, \"Synset('hosanna.n.01')\": 0.0, \"Synset('talk.n.01')\": 1.0, \"Synset('yak.n.01')\": 0.1, \"Synset('discussion.n.02')\": 0.9, \"Synset('hearing.n.05')\": 0.5, \"Synset('examination.n.02')\": 0.3, \"Synset('pop_quiz.n.01')\": 0.4, \"Synset('report.n.01')\": 0.9, \"Synset('assay.n.03')\": 0.1, \"Synset('change.n.01')\": 1.0, \"Synset('start.n.01')\": 1.0, \"Synset('thelarche.n.01')\": 0.0, \"Synset('meeting.n.03')\": 1.0, \"Synset('conjunction.n.05')\": 0.2, \"Synset('fall.n.06')\": 1.0, \"Synset('footrace.n.01')\": 0.0, \"Synset('obstacle_race.n.01')\": 0.2, \"Synset('wish.n.01')\": 1.0, \"Synset('velleity.n.01')\": 0.0, \"Synset('hope.n.02')\": 1.0, \"Synset('sanguinity.n.01')\": 0.0, \"Synset('vitamin_pill.n.01')\": 0.4, \"Synset('body.n.02')\": 1.0, \"Synset('meritocracy.n.01')\": 0.1, \"Synset('company.n.01')\": 1.0, \"Synset('business.n.01')\": 0.9, \"Synset('magazine.n.03')\": 1.0, \"Synset('church.n.01')\": 1.0, \"Synset('eastern_church.n.02')\": 0.0, \"Synset('lineage.n.01')\": 0.0, \"Synset('businessmen.n.01')\": 0.7, \"Synset('qing.n.01')\": 0.0, \"Synset('house.n.05')\": 1.0, \"Synset('nation.n.02')\": 1.0, \"Synset('world_power.n.01')\": 0.2, \"Synset('hegemon.n.01')\": 0.0, \"Synset('multitude.n.03')\": 0.4, \"Synset('military_unit.n.01')\": 0.3, \"Synset('police.n.01')\": 1.0, \"Synset('schutzstaffel.n.01')\": 0.0, \"Synset('work_force.n.01')\": 0.4, \"Synset('ship's_company.n.01')\": 0.0, \"Synset('viewing_audience.n.01')\": 0.2, \"Synset('club.n.02')\": 1.0, \"Synset('family.n.08')\": 1.0, \"Synset('koinonia.n.01')\": 0.0, \"Synset('athenaeum.n.01')\": 0.0, \"Synset('union_shop.n.01')\": 0.1, \"Synset('set.n.05')\": 1.0, \"Synset('mafia.n.01')\": 0.4, \"Synset('syndicate.n.01')\": 0.1, \"Synset('party.n.01')\": 1.0, \"Synset('american_labor_party.n.01')\": 0.0, \"Synset('table.n.01')\": 1.0, \"Synset('calendar.n.03')\": 1.0, \"Synset('dail_eireann.n.01')\": 0.0, \"Synset('board.n.01')\": 1.0, \"Synset('committee.n.01')\": 0.4, \"Synset('shadow_cabinet.n.01')\": 0.0, \"Synset('draft_board.n.01')\": 0.2, \"Synset('infantry.n.01')\": 0.2, \"Synset('paratroops.n.01')\": 0.0, \"Synset('contingent.n.02')\": 0.2, \"Synset('left.n.02')\": 1.0, \"Synset('system.n.02')\": 0.9, \"Synset('movement.n.04')\": 1.0, \"Synset('art_nouveau.n.01')\": 0.0, \"Synset('side.n.04')\": 1.0, \"Synset('bridgehead.n.02')\": 0.0, \"Synset('country.n.02')\": 1.0, \"Synset('domain.n.02')\": 0.6, \"Synset('fiefdom.n.01')\": 0.0, \"Synset('field.n.01')\": 1.0, \"Synset('playing_field.n.02')\": 0.6, \"Synset('ground.n.05')\": 0.8, \"Synset('peak.n.04')\": 0.6, \"Synset('pinnacle.n.03')\": 0.1, \"Synset('military_position.n.01')\": 0.2, \"Synset('post.n.01')\": 1.0, \"Synset('region.n.01')\": 1.0, \"Synset('town.n.01')\": 1.0, \"Synset('burg.n.01')\": 0.0, \"Synset('greece.n.02')\": 0.5, \"Synset('libidinal_energy.n.01')\": 0.0, \"Synset('bed.n.03')\": 1.0, \"Synset('continental_shelf.n.01')\": 0.0, \"Synset('island.n.01')\": 1.0, \"Synset('land.n.02')\": 1.0, \"Synset('mire.n.01')\": 0.0, \"Synset('mull.n.01')\": 0.1, \"Synset('narrow.n.01')\": 0.5, \"Synset('natural_order.n.01')\": 0.2, \"Synset('promontory.n.01')\": 0.0, \"Synset('south_sea_islands.n.01')\": 0.2, \"Synset('strait.n.01')\": 0.2, \"Synset('universe.n.01')\": 1.0, \"Synset('godhead.n.01')\": 0.0, \"Synset('trinity.n.02')\": 0.2, \"Synset('succubus.n.01')\": 0.1, \"Synset('spirit.n.04')\": 1.0, \"Synset('swiss.n.01')\": 0.5, \"Synset('administrator.n.01')\": 0.6, \"Synset('affine.n.01')\": 0.1, \"Synset('alcalde.n.01')\": 0.0, \"Synset('ally.n.02')\": 0.3, \"Synset('anagnost.n.01')\": 0.0, \"Synset('backpacker.n.01')\": 0.1, \"Synset('blood_brother.n.02')\": 0.3, \"Synset('bridge_player.n.01')\": 0.0, \"Synset('broadcast_journalist.n.01')\": 0.2, \"Synset('brownie.n.01')\": 0.3, \"Synset('buddy.n.01')\": 0.6, \"Synset('chancellor.n.03')\": 0.0, \"Synset('child.n.01')\": 1.0, \"Synset('colonialist.n.01')\": 0.2, \"Synset('colonizer.n.01')\": 0.1, \"Synset('contractor.n.02')\": 0.1, \"Synset('founder.n.02')\": 0.8, \"Synset('friend.n.01')\": 1.0, \"Synset('gibson_girl.n.01')\": 0.0, \"Synset('head.n.04')\": 1.0, \"Synset('helpmate.n.01')\": 0.3, \"Synset('hired_hand.n.01')\": 0.0, \"Synset('judge.n.01')\": 0.9, \"Synset('juggernaut.n.01')\": 0.0, \"Synset('relative.n.01')\": 1.0, \"Synset('kin.n.01')\": 0.2, \"Synset('male_child.n.01')\": 0.9, \"Synset('power.n.05')\": 1.0, \"Synset('principal.n.02')\": 0.8, \"Synset('supporter.n.01')\": 0.7, \"Synset('terror.n.03')\": 0.9, \"Synset('weeder.n.01')\": 0.0, \"Synset('woman.n.01')\": 1.0, \"Synset('writer.n.01')\": 1.0, \"Synset('breeze.n.01')\": 0.2, \"Synset('breath.n.05')\": 0.9, \"Synset('glow.n.05')\": 0.5, \"Synset('light.n.01')\": 1.0, \"Synset('weather.n.01')\": 1.0, \"Synset('lady's_smock.n.01')\": 0.0, \"Synset('earning_per_share.n.01')\": 0.0, \"Synset('share.n.01')\": 1.0, \"Synset('undivided_interest.n.01')\": 0.0, \"Synset('price.n.02')\": 1.0, \"Synset('support_level.n.01')\": 0.2, \"Synset('sum.n.01')\": 1.0, \"Synset('support.n.06')\": 0.9, \"Synset('subsistence.n.01')\": 0.0, \"Synset('store.n.02')\": 1.0, \"Synset('provision.n.04')\": 0.1, \"Synset('money.n.01')\": 1.0, \"Synset('subsidization.n.01')\": 0.0, \"Synset('bill.n.03')\": 1.0, \"Synset('silver_certificate.n.01')\": 0.0, \"Synset('autolysis.n.01')\": 0.0, \"Synset('deposition.n.01')\": 0.1, \"Synset('increase.n.03')\": 0.8, \"Synset('natural_process.n.01')\": 0.3, \"Synset('stationary_stochastic_process.n.01')\": 0.0, \"Synset('couple.n.04')\": 1.0, \"Synset('doubleton.n.01')\": 0.0, \"Synset('thousand.n.01')\": 1.0, \"Synset('millenary.n.03')\": 0.3, \"Synset('compass_point.n.01')\": 0.1, \"Synset('southwest_by_west.n.01')\": 0.0, \"Synset('branch.n.03')\": 0.6, \"Synset('fork.n.03')\": 1.0, \"Synset('condition.n.01')\": 0.8, \"Synset('situation.n.02')\": 0.9, \"Synset('niche.n.01')\": 0.0, \"Synset('poverty_trap.n.01')\": 0.1, \"Synset('degree.n.02')\": 1.0, \"Synset('office.n.04')\": 1.0, \"Synset('executive_clemency.n.01')\": 0.0, \"Synset('place.n.10')\": 1.0, \"Synset('holy_order.n.01')\": 0.1, \"Synset('action.n.02')\": 1.0, \"Synset('running.n.03')\": 0.8, \"Synset('arrest.n.02')\": 0.8, \"Synset('logjam.n.01')\": 0.0, \"Synset('quickening.n.02')\": 0.1, \"Synset('blackwater_fever.n.01')\": 0.0, \"Synset('effect.n.06')\": 1.0, \"Synset('hairy_tongue.n.01')\": 0.0, \"Synset('esteem.n.01')\": 0.1, \"Synset('stature.n.01')\": 0.3, \"Synset('need.n.01')\": 0.9, \"Synset('urgency.n.01')\": 0.5, \"Synset('opportunity.n.01')\": 0.7, \"Synset('say.n.01')\": 1.0, \"Synset('sphere.n.01')\": 0.8, \"Synset('distaff.n.01')\": 0.1, \"Synset('tempestuousness.n.01')\": 0.0, \"Synset('chalcedony.n.01')\": 0.0, \"Synset('daub.n.01')\": 0.0, \"Synset('earth.n.02')\": 1.0, \"Synset('water.n.01')\": 1.0, \"Synset('slush.n.01')\": 0.0, \"Synset('time.n.03')\": 1.0, \"Synset('incipiency.n.01')\": 0.0, \"Synset('death.n.04')\": 1.0, \"Synset('grave.n.01')\": 0.7, \"Synset('day.n.04')\": 1.0, \"Synset('afternoon.n.01')\": 1.0, \"Synset('bedtime.n.01')\": 0.8, \"Synset('early-morning_hour.n.01')\": 0.0, \"Synset('week.n.01')\": 1.0, \"Synset('holy_year.n.01')\": 0.1, \"Synset('year.n.01')\": 1.0, \"Synset('calendar_month.n.01')\": 0.5, \"Synset('adar.n.01')\": 0.0, \"Synset('hour.n.02')\": 1.0, \"Synset('moment.n.01')\": 1.0, \"Synset('pinpoint.n.01')\": 0.0, \"Synset('beginning.n.02')\": 0.9, \"Synset('return_on_invested_capital.n.01')\": 0.0, \"Synset('rate.n.01')\": 0.9, \"Synset('flower.n.03')\": 1.0, \"Synset('golden_age.n.01')\": 0.3, \"Synset('shiva.n.01')\": 0.0, \"Synset('snap.n.02')\": 0.4}\n",
                        "[\"Synset('whole.n.02')\", \"Synset('motivation.n.01')\", \"Synset('discovery.n.01')\", \"Synset('sleeper.n.09')\", \"Synset('hit.n.03')\", \"Synset('shooting.n.01')\", \"Synset('enfilade.n.01')\", \"Synset('designation.n.03')\", \"Synset('manipulation.n.01')\", \"Synset('mind_game.n.02')\", \"Synset('decision.n.01')\", \"Synset('ruse.n.01')\", \"Synset('means.n.01')\", \"Synset('royal_road.n.01')\", \"Synset('step.n.03')\", \"Synset('trip.n.06')\", \"Synset('motion.n.03')\", \"Synset('inversion.n.08')\", \"Synset('activity.n.01')\", \"Synset('habit.n.02')\", \"Synset('second_nature.n.01')\", \"Synset('dance_step.n.01')\", \"Synset('glissade.n.01')\", \"Synset('position.n.06')\", \"Synset('associateship.n.01')\", \"Synset('care.n.01')\", \"Synset('myringotomy.n.01')\", \"Synset('desensitization_technique.n.01')\", \"Synset('function.n.03')\", \"Synset('behalf.n.01')\", \"Synset('sea-duty.n.01')\", \"Synset('attempt.n.01')\", \"Synset('contribution.n.01')\", \"Synset('end.n.11')\", \"Synset('striving.n.01')\", \"Synset('test.n.05')\", \"Synset('double_blind.n.01')\", \"Synset('clasp.n.02')\", \"Synset('hammerlock.n.01')\", \"Synset('reaction.n.03')\", \"Synset('meteortropism.n.01')\", \"Synset('look.n.02')\", \"Synset('eye-beaming.n.01')\", \"Synset('education.n.01')\", \"Synset('course.n.01')\", \"Synset('art_class.n.01')\", \"Synset('workshop.n.02')\", \"Synset('arboriculture.n.01')\", \"Synset('use.n.01')\", \"Synset('play.n.06')\", \"Synset('exploitation.n.01')\", \"Synset('unitization.n.04')\", \"Synset('military_action.n.01')\", \"Synset('armageddon.n.02')\", \"Synset('violence.n.01')\", \"Synset('war.n.01')\", \"Synset('fire.n.02')\", \"Synset('counterbombardment.n.01')\", \"Synset('jihad.n.01')\", \"Synset('procedure.n.01')\", \"Synset('rest.n.02')\", \"Synset('caravanning.n.01')\", \"Synset('commercial_enterprise.n.02')\", \"Synset('deal.n.01')\", \"Synset('penny_ante.n.01')\", \"Synset('management.n.01')\", \"Synset('toleration.n.02')\", \"Synset('road_rage.n.01')\", \"Synset('fight.n.02')\", \"Synset('affray.n.02')\", \"Synset('aid.n.02')\", \"Synset('auspices.n.01')\", \"Synset('crest.n.05')\", \"Synset('chine.n.02')\", \"Synset('homo.n.02')\", \"Synset('homo_sapiens.n.01')\", \"Synset('avenue.n.02')\", \"Synset('back.n.08')\", \"Synset('base.n.08')\", \"Synset('bell_gable.n.01')\", \"Synset('bilge.n.02')\", \"Synset('book.n.02')\", \"Synset('boucle.n.01')\", \"Synset('brass_monkey.n.01')\", \"Synset('caddy.n.01')\", \"Synset('can.n.01')\", \"Synset('cantle.n.01')\", \"Synset('car.n.01')\", \"Synset('cocktail_lounge.n.01')\", \"Synset('conservatory.n.02')\", \"Synset('cusp.n.01')\", \"Synset('dwelling.n.01')\", \"Synset('fabric.n.01')\", \"Synset('fresco.n.01')\", \"Synset('front.n.04')\", \"Synset('handle.n.01')\", \"Synset('hermitage.n.01')\", \"Synset('hose.n.03')\", \"Synset('ladies'_room.n.01')\", \"Synset('lighter.n.02')\", \"Synset('man.n.10')\", \"Synset('model_t.n.01')\", \"Synset('nose_cone.n.01')\", \"Synset('oeuvre.n.01')\", \"Synset('painting.n.01')\", \"Synset('palestra.n.01')\", \"Synset('picture.n.01')\", \"Synset('pommel.n.01')\", \"Synset('radiator_cap.n.01')\", \"Synset('recording_studio.n.01')\", \"Synset('room.n.01')\", \"Synset('safety_match.n.01')\", \"Synset('sawhorse.n.01')\", \"Synset('school.n.02')\", \"Synset('side_yard.n.01')\", \"Synset('stopcock.n.01')\", \"Synset('street.n.01')\", \"Synset('thing.n.04')\", \"Synset('toilet.n.01')\", \"Synset('top.n.09')\", \"Synset('trestle.n.02')\", \"Synset('viewgraph.n.01')\", \"Synset('wall.n.01')\", \"Synset('way.n.06')\", \"Synset('white.n.11')\", \"Synset('workplace.n.01')\", \"Synset('activeness.n.02')\", \"Synset('dynamism.n.03')\", \"Synset('air.n.03')\", \"Synset('vibration.n.04')\", \"Synset('looseness.n.05')\", \"Synset('wiggliness.n.01')\", \"Synset('determination.n.02')\", \"Synset('doggedness.n.01')\", \"Synset('sanctimoniousness.n.01')\", \"Synset('sound.n.01')\", \"Synset('voice.n.01')\", \"Synset('luminosity.n.01')\", \"Synset('incandescence.n.02')\", \"Synset('pace.n.03')\", \"Synset('promptness.n.01')\", \"Synset('shape.n.01')\", \"Synset('oblateness.n.01')\", \"Synset('probability.n.01')\", \"Synset('cross_section.n.03')\", \"Synset('amount.n.02')\", \"Synset('quantity.n.02')\", \"Synset('number.n.01')\", \"Synset('multitudinousness.n.01')\", \"Synset('monetary_value.n.01')\", \"Synset('assessment.n.03')\", \"Synset('interest.n.03')\", \"Synset('shrillness.n.01')\", \"Synset('human_body.n.01')\", \"Synset('person.n.02')\", \"Synset('eye.n.01')\", \"Synset('peeper.n.02')\", \"Synset('process.n.05')\", \"Synset('spinal_column.n.01')\", \"Synset('face.n.01')\", \"Synset('countenance.n.03')\", \"Synset('ability.n.02')\", \"Synset('puppetry.n.01')\", \"Synset('art.n.03')\", \"Synset('bonding.n.02')\", \"Synset('convention.n.02')\", \"Synset('protocol.n.03')\", \"Synset('attention.n.01')\", \"Synset('advertence.n.01')\", \"Synset('feeling.n.06')\", \"Synset('sprachgefuhl.n.01')\", \"Synset('touch.n.10')\", \"Synset('cutaneous_sensation.n.01')\", \"Synset('line.n.29')\", \"Synset('rubicon.n.02')\", \"Synset('thinking.n.01')\", \"Synset('basis.n.02')\", \"Synset('meat_and_potatoes.n.01')\", \"Synset('nature_study.n.01')\", \"Synset('estimate.n.01')\", \"Synset('guess.n.02')\", \"Synset('real_world.n.01')\", \"Synset('reading.n.03')\", \"Synset('fact.n.01')\", \"Synset('detail.n.01')\", \"Synset('particular.n.01')\", \"Synset('nook_and_cranny.n.01')\", \"Synset('circumstance.n.03')\", \"Synset('justification.n.01')\", \"Synset('source.n.03')\", \"Synset('muse.n.02')\", \"Synset('judgment.n.01')\", \"Synset('predetermination.n.02')\", \"Synset('kind.n.01')\", \"Synset('stripe.n.04')\", \"Synset('rule.n.11')\", \"Synset('lateral_thinking.n.01')\", \"Synset('quality.n.03')\", \"Synset('texture.n.02')\", \"Synset('value.n.01')\", \"Synset('plot_element.n.01')\", \"Synset('point.n.01')\", \"Synset('strange_attractor.n.01')\", \"Synset('part.n.09')\", \"Synset('hypothesis.n.02')\", \"Synset('plot.n.01')\", \"Synset('intrigue.n.01')\", \"Synset('principle.n.01')\", \"Synset('pillar_of_islam.n.01')\", \"Synset('meaning.n.02')\", \"Synset('kernel.n.03')\", \"Synset('stuff.n.07')\", \"Synset('undertone.n.02')\", \"Synset('exemplar.n.01')\", \"Synset('character.n.04')\", \"Synset('soubrette.n.02')\", \"Synset('form.n.03')\", \"Synset('strand.n.01')\", \"Synset('prodigy.n.03')\", \"Synset('promise.n.02')\", \"Synset('rainbow.n.02')\", \"Synset('thought.n.03')\", \"Synset('mainstream.n.01')\", \"Synset('experience.n.02')\", \"Synset('respect.n.03')\", \"Synset('reputation.n.03')\", \"Synset('newspaper.n.01')\", \"Synset('gazette.n.01')\", \"Synset('word.n.01')\", \"Synset('demonstrative_pronoun.n.01')\", \"Synset('heading.n.01')\", \"Synset('title.n.01')\", \"Synset('argument.n.05')\", \"Synset('writing.n.03')\", \"Synset('margin.n.05')\", \"Synset('space.n.07')\", \"Synset('conclusion.n.08')\", \"Synset('peroration.n.02')\", \"Synset('handwriting.n.01')\", \"Synset('calligraphy.n.01')\", \"Synset('jotter.n.01')\", \"Synset('card.n.08')\", \"Synset('opinion.n.04')\", \"Synset('plea_bargain.n.01')\", \"Synset('instruction.n.04')\", \"Synset('link.n.06')\", \"Synset('issue.n.02')\", \"Synset('edition.n.03')\", \"Synset('message.n.02')\", \"Synset('movie.n.01')\", \"Synset('scene.n.04')\", \"Synset('outtake.n.01')\", \"Synset('spaghetti_western.n.01')\", \"Synset('record.n.05')\", \"Synset('news.n.01')\", \"Synset('latest.n.01')\", \"Synset('sign.n.01')\", \"Synset('token.n.02')\", \"Synset('higher_law.n.01')\", \"Synset('impudence.n.01')\", \"Synset('answer.n.01')\", \"Synset('rescript.n.01')\", \"Synset('obiter_dictum.n.02')\", \"Synset('fun.n.02')\", \"Synset('jocosity.n.01')\", \"Synset('direction.n.06')\", \"Synset('numeral.n.01')\", \"Synset('antilogarithm.n.01')\", \"Synset('act.n.04')\", \"Synset('show-stopper.n.01')\", \"Synset('artwork.n.01')\", \"Synset('illustration.n.01')\", \"Synset('music.n.01')\", \"Synset('theme.n.03')\", \"Synset('diminution.n.02')\", \"Synset('secondo.n.01')\", \"Synset('accompaniment.n.02')\", \"Synset('descant.n.01')\", \"Synset('meter.n.03')\", \"Synset('anapest.n.01')\", \"Synset('speech.n.02')\", \"Synset('cry.n.01')\", \"Synset('hosanna.n.01')\", \"Synset('talk.n.01')\", \"Synset('yak.n.01')\", \"Synset('discussion.n.02')\", \"Synset('hearing.n.05')\", \"Synset('examination.n.02')\", \"Synset('pop_quiz.n.01')\", \"Synset('report.n.01')\", \"Synset('assay.n.03')\", \"Synset('change.n.01')\", \"Synset('start.n.01')\", \"Synset('thelarche.n.01')\", \"Synset('meeting.n.03')\", \"Synset('conjunction.n.05')\", \"Synset('fall.n.06')\", \"Synset('footrace.n.01')\", \"Synset('obstacle_race.n.01')\", \"Synset('wish.n.01')\", \"Synset('velleity.n.01')\", \"Synset('hope.n.02')\", \"Synset('sanguinity.n.01')\", \"Synset('vitamin_pill.n.01')\", \"Synset('body.n.02')\", \"Synset('meritocracy.n.01')\", \"Synset('company.n.01')\", \"Synset('business.n.01')\", \"Synset('magazine.n.03')\", \"Synset('church.n.01')\", \"Synset('eastern_church.n.02')\", \"Synset('lineage.n.01')\", \"Synset('businessmen.n.01')\", \"Synset('qing.n.01')\", \"Synset('house.n.05')\", \"Synset('nation.n.02')\", \"Synset('world_power.n.01')\", \"Synset('hegemon.n.01')\", \"Synset('multitude.n.03')\", \"Synset('military_unit.n.01')\", \"Synset('police.n.01')\", \"Synset('schutzstaffel.n.01')\", \"Synset('work_force.n.01')\", \"Synset('ship's_company.n.01')\", \"Synset('viewing_audience.n.01')\", \"Synset('club.n.02')\", \"Synset('family.n.08')\", \"Synset('koinonia.n.01')\", \"Synset('athenaeum.n.01')\", \"Synset('union_shop.n.01')\", \"Synset('set.n.05')\", \"Synset('mafia.n.01')\", \"Synset('syndicate.n.01')\", \"Synset('party.n.01')\", \"Synset('american_labor_party.n.01')\", \"Synset('table.n.01')\", \"Synset('calendar.n.03')\", \"Synset('dail_eireann.n.01')\", \"Synset('board.n.01')\", \"Synset('committee.n.01')\", \"Synset('shadow_cabinet.n.01')\", \"Synset('draft_board.n.01')\", \"Synset('infantry.n.01')\", \"Synset('paratroops.n.01')\", \"Synset('contingent.n.02')\", \"Synset('left.n.02')\", \"Synset('system.n.02')\", \"Synset('movement.n.04')\", \"Synset('art_nouveau.n.01')\", \"Synset('side.n.04')\", \"Synset('bridgehead.n.02')\", \"Synset('country.n.02')\", \"Synset('domain.n.02')\", \"Synset('fiefdom.n.01')\", \"Synset('field.n.01')\", \"Synset('playing_field.n.02')\", \"Synset('ground.n.05')\", \"Synset('peak.n.04')\", \"Synset('pinnacle.n.03')\", \"Synset('military_position.n.01')\", \"Synset('post.n.01')\", \"Synset('region.n.01')\", \"Synset('town.n.01')\", \"Synset('burg.n.01')\", \"Synset('greece.n.02')\", \"Synset('libidinal_energy.n.01')\", \"Synset('bed.n.03')\", \"Synset('continental_shelf.n.01')\", \"Synset('island.n.01')\", \"Synset('land.n.02')\", \"Synset('mire.n.01')\", \"Synset('mull.n.01')\", \"Synset('narrow.n.01')\", \"Synset('natural_order.n.01')\", \"Synset('promontory.n.01')\", \"Synset('south_sea_islands.n.01')\", \"Synset('strait.n.01')\", \"Synset('universe.n.01')\", \"Synset('godhead.n.01')\", \"Synset('trinity.n.02')\", \"Synset('succubus.n.01')\", \"Synset('spirit.n.04')\", \"Synset('swiss.n.01')\", \"Synset('administrator.n.01')\", \"Synset('affine.n.01')\", \"Synset('alcalde.n.01')\", \"Synset('ally.n.02')\", \"Synset('anagnost.n.01')\", \"Synset('backpacker.n.01')\", \"Synset('blood_brother.n.02')\", \"Synset('bridge_player.n.01')\", \"Synset('broadcast_journalist.n.01')\", \"Synset('brownie.n.01')\", \"Synset('buddy.n.01')\", \"Synset('chancellor.n.03')\", \"Synset('child.n.01')\", \"Synset('colonialist.n.01')\", \"Synset('colonizer.n.01')\", \"Synset('contractor.n.02')\", \"Synset('founder.n.02')\", \"Synset('friend.n.01')\", \"Synset('gibson_girl.n.01')\", \"Synset('head.n.04')\", \"Synset('helpmate.n.01')\", \"Synset('hired_hand.n.01')\", \"Synset('judge.n.01')\", \"Synset('juggernaut.n.01')\", \"Synset('relative.n.01')\", \"Synset('kin.n.01')\", \"Synset('male_child.n.01')\", \"Synset('power.n.05')\", \"Synset('principal.n.02')\", \"Synset('supporter.n.01')\", \"Synset('terror.n.03')\", \"Synset('weeder.n.01')\", \"Synset('woman.n.01')\", \"Synset('writer.n.01')\", \"Synset('breeze.n.01')\", \"Synset('breath.n.05')\", \"Synset('glow.n.05')\", \"Synset('light.n.01')\", \"Synset('weather.n.01')\", \"Synset('lady's_smock.n.01')\", \"Synset('earning_per_share.n.01')\", \"Synset('share.n.01')\", \"Synset('undivided_interest.n.01')\", \"Synset('price.n.02')\", \"Synset('support_level.n.01')\", \"Synset('sum.n.01')\", \"Synset('support.n.06')\", \"Synset('subsistence.n.01')\", \"Synset('store.n.02')\", \"Synset('provision.n.04')\", \"Synset('money.n.01')\", \"Synset('subsidization.n.01')\", \"Synset('bill.n.03')\", \"Synset('silver_certificate.n.01')\", \"Synset('autolysis.n.01')\", \"Synset('deposition.n.01')\", \"Synset('increase.n.03')\", \"Synset('natural_process.n.01')\", \"Synset('stationary_stochastic_process.n.01')\", \"Synset('couple.n.04')\", \"Synset('doubleton.n.01')\", \"Synset('thousand.n.01')\", \"Synset('millenary.n.03')\", \"Synset('compass_point.n.01')\", \"Synset('southwest_by_west.n.01')\", \"Synset('branch.n.03')\", \"Synset('fork.n.03')\", \"Synset('condition.n.01')\", \"Synset('situation.n.02')\", \"Synset('niche.n.01')\", \"Synset('poverty_trap.n.01')\", \"Synset('degree.n.02')\", \"Synset('office.n.04')\", \"Synset('executive_clemency.n.01')\", \"Synset('place.n.10')\", \"Synset('holy_order.n.01')\", \"Synset('action.n.02')\", \"Synset('running.n.03')\", \"Synset('arrest.n.02')\", \"Synset('logjam.n.01')\", \"Synset('quickening.n.02')\", \"Synset('blackwater_fever.n.01')\", \"Synset('effect.n.06')\", \"Synset('hairy_tongue.n.01')\", \"Synset('esteem.n.01')\", \"Synset('stature.n.01')\", \"Synset('need.n.01')\", \"Synset('urgency.n.01')\", \"Synset('opportunity.n.01')\", \"Synset('say.n.01')\", \"Synset('sphere.n.01')\", \"Synset('distaff.n.01')\", \"Synset('tempestuousness.n.01')\", \"Synset('chalcedony.n.01')\", \"Synset('daub.n.01')\", \"Synset('earth.n.02')\", \"Synset('water.n.01')\", \"Synset('slush.n.01')\", \"Synset('time.n.03')\", \"Synset('incipiency.n.01')\", \"Synset('death.n.04')\", \"Synset('grave.n.01')\", \"Synset('day.n.04')\", \"Synset('afternoon.n.01')\", \"Synset('bedtime.n.01')\", \"Synset('early-morning_hour.n.01')\", \"Synset('week.n.01')\", \"Synset('holy_year.n.01')\", \"Synset('year.n.01')\", \"Synset('calendar_month.n.01')\", \"Synset('adar.n.01')\", \"Synset('hour.n.02')\", \"Synset('moment.n.01')\", \"Synset('pinpoint.n.01')\", \"Synset('beginning.n.02')\", \"Synset('return_on_invested_capital.n.01')\", \"Synset('rate.n.01')\", \"Synset('flower.n.03')\", \"Synset('golden_age.n.01')\", \"Synset('shiva.n.01')\", \"Synset('snap.n.02')\"]\n"
                    ]
                }
            ],
            "source": [
                "agreement_dict = dict(zip(df_merged_features_clean['Synset'], df_merged_features_clean['Agreement score']))\n",
                "print(agreement_dict)\n",
                "\n",
                "synset_with_agreement_list = list(df_merged_features_clean['Synset'])\n",
                "print(synset_with_agreement_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Estraiamo le colonne da usare come features.\n",
                "- Non viene usato 'Agreement score' come feature."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['In CBT', 'Normalized Depth', 'Normalized LCH Similarity', 'Normalized Frequency', 'Normalized Synonymy']\n"
                    ]
                }
            ],
            "source": [
                "if only_synset_with_agreement:\n",
                "    columns_names_to_compute_score = ['isHard mean', 'In CBT'] + [col for col in df_merged_features_clean.columns if col.startswith('Normalized')]\n",
                "else: \n",
                "    columns_names_to_compute_score = ['In CBT'] + [col for col in df_merged_features_clean.columns if col.startswith('Normalized') and col != 'Normalized timeDiffs mean']\n",
                "\n",
                "df_features_to_compute_score = df_merged_features_clean[columns_names_to_compute_score].copy()\n",
                "# print(df_features_to_compute_score[:5])\n",
                "print(df_features_to_compute_score.columns.to_list())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calcolo dei pesi delle features con ML"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Suddivisione dei dati in training e test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_training_test_sets(X, y, training_size):\n",
                "    test_size_perc = (100 - training_size)/100\n",
                "    \n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_perc, random_state=42)\n",
                "    \n",
                "    return X_train, X_test, y_train, y_test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### LR - LinearRegression Model\n",
                "\n",
                "Il modello di regressione lineare viene creato e addestrato senza alcuna ottimizzazione dei suoi parametri."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_linear_regression_model(X, y, training_size):\n",
                "    X_train, X_test, y_train, y_test = get_training_test_sets(X, y, training_size)\n",
                "    \n",
                "    model = LinearRegression()\n",
                "    \n",
                "    # Addestra il modello\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predici sul test set\n",
                "    predictions = model.predict(X_test)\n",
                "    \n",
                "    # Valuta le performance del modello\n",
                "    mse = root_mean_squared_error(y_test, predictions)\n",
                "    r2 = r2_score(y_test, predictions)\n",
                "    \n",
                "    return model, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### LR GridSearchCV\n",
                "Viene utilizzata la GridSearchCV di scikit-learn per cercare i migliori pesi per il modello di regressione lineare. La Grid Search esplora diverse combinazioni di pesi e restituisce il set di pesi che ottimizzano una metrica specifica, come l'errore quadratico medio (MSE) o il coefficiente di determinazione (R²). Viene quindi restituito il modello con i migliori parametri (best_model).\n",
                "\n",
                "Durante la Grid Search viene utilizzata la cross-validation con 5 fold (cv=5),quindi il training set viene diviso in 5 fold e il modello viene addestrato e validato su ciascun fold in modo iterativo. Ciò aiuta a valutare la performance del modello in modo più robusto."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_linear_regression_model_opt(X, y, training_size):\n",
                "    X_train, X_test, y_train, y_test = get_training_test_sets(X, y, training_size)\n",
                "    model = LinearRegression()\n",
                "\n",
                "    # Definisci i pesi da testare\n",
                "    param_grid = {\n",
                "        'fit_intercept': [True, False],\n",
                "    }\n",
                "\n",
                "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
                "\n",
                "    grid_search.fit(X_train, y_train)\n",
                "\n",
                "    best_model = grid_search.best_estimator_\n",
                "\n",
                "    # Predici sul set di test\n",
                "    predictions = best_model.predict(X_test)\n",
                "    \n",
                "    # Valuta le performance del modello\n",
                "    mse = root_mean_squared_error(y_test, predictions)\n",
                "    r2 = r2_score(y_test, predictions)\n",
                "\n",
                "    return best_model, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### LR - CV\n",
                "\n",
                "Per ottenere una stima più robusta delle performance del modello viene utilizzata la Cross Validation. Viene suddiviso il dataset in più fold e il modello viene addestrato/valutato su diverse combinazioni di dati di training e test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_linear_regression_model_with_cross_validation(X, y):\n",
                "    model = LinearRegression()\n",
                "    model.fit(X, y)\n",
                "\n",
                "    # CV 5 fold utilizzando MSE\n",
                "    cv_scores_mse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
                "    cv_scores_mse = -cv_scores_mse\n",
                "    mean_cv_score_mse = cv_scores_mse.mean()\n",
                "\n",
                "    # CV 5 fold utilizzando R²\n",
                "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
                "    mean_cv_score_r2 = cv_scores_r2.mean()\n",
                "    \n",
                "    return model, mean_cv_score_mse, mean_cv_score_r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Lasso model\n",
                "- La regressione lineare cerca di minimizzare solo la somma dei quadrati delle differenze, mentre la regressione Lasso aggiunge una penalità basata sulla somma degli importi assoluti dei coefficienti.\n",
                "- La regressione Lasso può portare alcuni coefficienti a diventare esattamente zero, fornendo quindi una forma di selezione delle variabili. In altre parole, la regressione Lasso può essere utilizzata per la selezione automatica delle feature, eliminando quelle meno importanti.\n",
                "- **alpha** è il parametro di **regolarizzazione**, che controlla il grado di riduzione dei pesi. Un valore più alto di alpha porta a una **maggiore riduzione dei pesi** e, quindi, a una maggiore sparsità."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_lasso_model(X, y, training_size):\n",
                "    X_train, X_test, y_train, y_test = get_training_test_sets(X, y, training_size)\n",
                "    \n",
                "    alpha = 0.001\n",
                "    model = Lasso(alpha=alpha)\n",
                "    \n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predici sul test set\n",
                "    predictions = model.predict(X_test)\n",
                "    \n",
                "    mse = root_mean_squared_error(y_test, predictions)\n",
                "    r2 = r2_score(y_test, predictions)\n",
                "    \n",
                "    return model, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Lasso con CV\n",
                "\n",
                "È stata utilizzata la Cross Validation (CV) con il modello Lasso per ottenere una stima più robusta delle performance del modello."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_lassoCV_model(X, y):\n",
                "    model = LassoCV(cv=5)\n",
                "    model.fit(X, y)\n",
                "    \n",
                "    # CV con MSE\n",
                "    cv_scores_mse = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
                "    mean_cv_score_mse = -cv_scores_mse.mean()\n",
                "    \n",
                "    # CV con R²\n",
                "    cv_scores_r2 = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
                "    mean_cv_score_r2 = cv_scores_r2.mean()\n",
                "\n",
                "    return model, mean_cv_score_mse, mean_cv_score_r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compute selected model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = [\n",
                "    'LinearRegression', \n",
                "    'LinearRegression-Cross-Validation', \n",
                "    'LinearRegression-Cross-Validation-GridSearchCV',\n",
                "    'Lasso',\n",
                "    'LassoCV']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(selected_model, dataframe, df_features, training_size):\n",
                "    X = df_features\n",
                "    y = dataframe['Agreement score']\n",
                "\n",
                "    model_functions = {\n",
                "        models[0]: lambda: compute_linear_regression_model(X, y, training_size),\n",
                "        models[1]: lambda: compute_linear_regression_model_with_cross_validation(X, y),\n",
                "        models[2]: lambda: compute_linear_regression_model_opt(X, y, training_size),\n",
                "        models[3]: lambda: compute_lasso_model(X, y, training_size),\n",
                "        models[4]: lambda: compute_lassoCV_model(X, y),\n",
                "    }\n",
                "    \n",
                "    if selected_model in model_functions:\n",
                "        model, mse, r2 = model_functions[selected_model]()\n",
                "        print(f'Model: {model}')\n",
                "        print(f'Mean Squared Error: {mse}')\n",
                "        print(f'R-squared: {r2}')\n",
                "        return model, mse, r2\n",
                "    else:\n",
                "        raise ValueError(\"Model not recognized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pesi delle features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model_weights(selected_model, df_with_agreement, df_features, training_size):\n",
                "    model, _, _ = train_model(selected_model, df_with_agreement, df_features, training_size)\n",
                "    \n",
                "    weights = model.coef_\n",
                "    \n",
                "    feature_weights_dict = dict(zip(df_features.columns, weights))\n",
                "    print(\"\\nFeature Weights:\")\n",
                "    for feature, weight in feature_weights_dict.items():\n",
                "        print(f\"{feature}: {weight}\")\n",
                "\n",
                "    # Normalizzazione dei pesi tra 0 e 1\n",
                "    weights_normalized = scaler.fit_transform(weights.reshape(-1, 1)).flatten()\n",
                "    total_weight = sum(weights_normalized)\n",
                "    weights_normalized /= total_weight\n",
                "    feature_weights_normalized_dict = dict(zip(df_features.columns, weights_normalized))\n",
                "    print(\"\\nNormalized Feature Weights:\")\n",
                "    for feature, weight in feature_weights_normalized_dict.items():\n",
                "        print(f\"{feature}: {weight}\")\n",
                "    \n",
                "    return feature_weights_dict, feature_weights_normalized_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: LassoCV(cv=5)\n",
                        "Mean Squared Error: 0.04957001842386431\n",
                        "R-squared: 0.7212978104397216\n",
                        "\n",
                        "Feature Weights:\n",
                        "In CBT: 0.32494178147225666\n",
                        "Normalized Depth: 0.191781665045967\n",
                        "Normalized LCH Similarity: 0.0\n",
                        "Normalized Frequency: 0.576019074999143\n",
                        "Normalized Synonymy: -0.0\n",
                        "\n",
                        "Normalized Feature Weights:\n",
                        "In CBT: 0.29736353722288317\n",
                        "Normalized Depth: 0.1755048982441552\n",
                        "Normalized LCH Similarity: 0.0\n",
                        "Normalized Frequency: 0.5271315645329616\n",
                        "Normalized Synonymy: 0.0\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "({'In CBT': 0.32494178147225666,\n",
                            "  'Normalized Depth': 0.191781665045967,\n",
                            "  'Normalized LCH Similarity': 0.0,\n",
                            "  'Normalized Frequency': 0.576019074999143,\n",
                            "  'Normalized Synonymy': -0.0},\n",
                            " {'In CBT': 0.29736353722288317,\n",
                            "  'Normalized Depth': 0.1755048982441552,\n",
                            "  'Normalized LCH Similarity': 0.0,\n",
                            "  'Normalized Frequency': 0.5271315645329616,\n",
                            "  'Normalized Synonymy': 0.0})"
                        ]
                    },
                    "execution_count": 54,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "selected_model = models[4]\n",
                "get_model_weights(selected_model, df_merged_features_clean, df_features_to_compute_score, 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calcolo basicness scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_basicness_scores(selected_model, df_cleaned, df_features, df_complete, columns_names_to_compute_score, only_synset_with_agreement, training_size):\n",
                "    model, _, _ = train_model(selected_model, df_cleaned, df_features, training_size)\n",
                "    \n",
                "    if not only_synset_with_agreement:\n",
                "        df_features = df_complete[columns_names_to_compute_score].copy()\n",
                "        final_df = df_complete\n",
                "    else:\n",
                "        final_df = df_cleaned\n",
                "        \n",
                "    # Predici sul dataset completo (non più suddividendo in train e test set)\n",
                "    final_df['Basicness score'] = model.predict(df_features)\n",
                "    \n",
                "    # Normalizzazione\n",
                "    final_df['Basicness score'] = scaler.fit_transform(final_df[['Basicness score']])\n",
                "    \n",
                "    return final_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: LassoCV(cv=5)\n",
                        "Mean Squared Error: 0.04957001842386431\n",
                        "R-squared: 0.7212978104397216\n",
                        "                           Synset  Depth  Normalized Depth  Agreement score   \n",
                        "0           Synset('entity.n.01')      0          1.000000              NaN  \\\n",
                        "1  Synset('physical_entity.n.01')      1          0.947368              NaN   \n",
                        "2      Synset('abstraction.n.06')      1          0.947368              NaN   \n",
                        "3            Synset('thing.n.12')      2          0.894737              NaN   \n",
                        "4           Synset('object.n.01')      2          0.894737              NaN   \n",
                        "\n",
                        "   isHard mean  timeDiffs mean  Normalized timeDiffs mean  LCH Similarity   \n",
                        "0          NaN             NaN                        NaN        3.637586  \\\n",
                        "1          NaN             NaN                        NaN        2.944439   \n",
                        "2          NaN             NaN                        NaN        2.944439   \n",
                        "3          NaN             NaN                        NaN        2.538974   \n",
                        "4          NaN             NaN                        NaN        2.538974   \n",
                        "\n",
                        "   Normalized LCH Similarity  Frequency  Normalized Frequency  Synonymy   \n",
                        "0                   1.000000   0.000013              0.765746         1  \\\n",
                        "1                   0.809449   0.000011              0.661326         1   \n",
                        "2                   0.809449   0.000002              0.116022         2   \n",
                        "3                   0.697983   0.000525              1.000000         1   \n",
                        "4                   0.697983   0.000036              1.000000         2   \n",
                        "\n",
                        "   Normalized Synonymy  In CBT  Basicness score  \n",
                        "0             1.000000    True         0.883604  \n",
                        "1             1.000000   False         0.515154  \n",
                        "2             0.962963    True         0.525257  \n",
                        "3             1.000000    True         0.990589  \n",
                        "4             0.962963    True         0.990589  \n"
                    ]
                }
            ],
            "source": [
                "selected_model = models[4]\n",
                "\n",
                "df_basicness_scores = compute_basicness_scores(selected_model, df_merged_features_clean, df_features_to_compute_score, df_merged_features, columns_names_to_compute_score, only_synset_with_agreement, 80)\n",
                "print(df_basicness_scores[:5])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "basicness_scores_dict = dict(zip(df_basicness_scores['Synset'], df_basicness_scores['Basicness score']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\"Synset('entity.n.01')\": 0.8836042933890997, \"Synset('physical_entity.n.01')\": 0.5151537719458408, \"Synset('abstraction.n.06')\": 0.5252568871817799, \"Synset('thing.n.12')\": 0.9905890401394974, \"Synset('object.n.01')\": 0.9905890401394974, \"Synset('whole.n.02')\": 0.9811780802789947, \"Synset('congener.n.03')\": 0.13321149391212883, \"Synset('living_thing.n.01')\": 0.6688066305700457, \"Synset('organism.n.01')\": 0.24353558321234836, \"Synset('benthos.n.02')\": 0.11452977187045935}\n"
                    ]
                }
            ],
            "source": [
                "primi_10_valori = {k: basicness_scores_dict[k] for k in list(basicness_scores_dict)[:10]}\n",
                "print(primi_10_valori)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Punteggio di basicness per un synset specifico:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.9811780802789947\n"
                    ]
                }
            ],
            "source": [
                "synset = str(wordnet.synset('whole.n.02'))\n",
                "score = basicness_scores_dict.get(synset, 'Synset non classificato')\n",
                "print(score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[0.52, 0.47, 0.11, 0.67, 0.24, 0.11, 0.6, 0.11, 0.12, 0.22]\n"
                    ]
                }
            ],
            "source": [
                "syn_list1 = [\"bank\", \"lamp\", \"dog\", \"wine\", \"water\", \"pan\", \"cloud\", \"drill\", \"entity\", \"relation\"]\n",
                "syn_list2 = [\"entity\", \"life\", \"storm\", \"action\", \"docking\", \"town\", \"watt\", \"capillarity\", \"work\", \"taxodiaceae\"]\n",
                "syn_list3 = [\"animal\", \"dress\", \"pen\", \"ossuary\", \"engine\", \"object\", \"pulsation\", \"plant\", \"enclosure\", \"guide\"]\n",
                "syn_list4 = [\"collection\", \"taxodiaceae\", \"drill\", \"perennial\", \"cat\", \"tree\", \"biont\", \"dog\", \"book\", \"can\"]\n",
                "syn_list5 = [\"abstraction\", \"heterotroph\", \"organism\", \"pine\", \"animal\", \"soup\", \"benthos\", \"mountain\", \"poor\", \"economy\"]\n",
                "syn_list6 = [\"physical_entity\", \"abstraction\", \"congener\", \"living_thing\", \"organism\", \"benthos\", \"dwarf\", \"heterotroph\", \"biont\", \"causal_agent\"]\n",
                "selected_syn_list = syn_list6\n",
                "\n",
                "score_list = []\n",
                "for word in selected_syn_list:\n",
                "    synset = str(wordnet.synset(f'{word}.n.01'))\n",
                "    score = basicness_scores_dict.get(synset, 'Synset non classificato')\n",
                "    score_list.append(round(score, 2))\n",
                "    \n",
                "print(score_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calcolo threshold\n",
                "Si desidera calcolare una soglia in base alla quale classificare un termine come 'basic' o 'advanced'.  \n",
                "Per determinare la soglia viene usato il *gold standard* costituito da 10 annotatori umani che hanno classificato 504 synset.  \n",
                "Si ha una soglia migliore quanti più synset sono stati classificati allo stesso modo dall'algoritmo rispetto al gold standard.\n",
                "- La soglia migliore trovata è 0.6990000000000003, con un totale di 467 synset su 504 classificati come nel gold standard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_threshold(basicness_scores_dict, agreement_dict):\n",
                "    total_synset_with_agreem = len(agreement_dict.items())\n",
                "    best_count = 0\n",
                "    count = 0\n",
                "    threshold = 0.3\n",
                "    best_threshold = threshold\n",
                "\n",
                "    while threshold < 1:\n",
                "        for syn_with_agreement, agreement_score in agreement_dict.items():\n",
                "            basicness_score = basicness_scores_dict.get(syn_with_agreement, 'Synset non classificato')\n",
                "            \n",
                "            basicness_score_binary = 1 if basicness_score >= threshold else 0\n",
                "            agreement_score_binary = 1 if agreement_score >= threshold else 0\n",
                "            \n",
                "            if agreement_score_binary == basicness_score_binary:\n",
                "                count += 1\n",
                "        \n",
                "        if best_count <= count: \n",
                "            best_count = count\n",
                "            best_threshold = threshold\n",
                "        \n",
                "        threshold += 0.001\n",
                "        count = 0\n",
                "\n",
                "    if total_synset_with_agreem == best_count:\n",
                "        print('Tutti i', total_synset_with_agreem, 'synset sono stati classificati correttamente.')\n",
                "    else:\n",
                "        print('Su', total_synset_with_agreem, 'synset del gold standard, solo', best_count, 'sono stati classificati correttamente')\n",
                "    \n",
                "    return best_threshold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Su 504 synset del gold standard, solo 467 sono stati classificati correttamente\n",
                        "best threshold found: 0.6990000000000003\n"
                    ]
                }
            ],
            "source": [
                "threshold = compute_threshold(basicness_scores_dict, agreement_dict)\n",
                "print('best threshold found:', threshold)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_answer(df_basicness_scores, basicness_scores_dict, agreement_dict):\n",
                "    computed_answers_list = []\n",
                "    threshold = compute_threshold(basicness_scores_dict, agreement_dict)\n",
                "    \n",
                "    for _, computed_score in basicness_scores_dict.items():\n",
                "        basic_or_advance_computed_score = 'basic' if computed_score >= threshold else 'advanced'\n",
                "        computed_answers_list.append(basic_or_advance_computed_score)\n",
                "    \n",
                "    agreement_answers_list = []\n",
                "    for agreem_score in df_basicness_scores['Agreement score']:\n",
                "        if pd.isna(agreem_score):\n",
                "            basic_or_advance_agreem_score = None\n",
                "        else:\n",
                "            basic_or_advance_agreem_score = 'basic' if agreem_score >= threshold else 'advanced'\n",
                "        agreement_answers_list.append(basic_or_advance_agreem_score)\n",
                "    return computed_answers_list, agreement_answers_list"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Riorganizzazione delle colonne del dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Su 504 synset del gold standard, solo 467 sono stati classificati correttamente\n"
                    ]
                }
            ],
            "source": [
                "computed_answers_list, agreement_answers_list = compute_answer(df_basicness_scores, basicness_scores_dict, agreement_dict)\n",
                "df_features_with_answers = df_basicness_scores.copy()\n",
                "\n",
                "\n",
                "df_columns_names_list = df_features_with_answers.columns.tolist()\n",
                "\n",
                "df_columns_names_list.remove('Agreement score')\n",
                "col_index_basicness = df_columns_names_list.index('Basicness score')\n",
                "df_columns_names_list.insert(col_index_basicness + 1, 'Agreement score')\n",
                "df_features_with_answers = df_features_with_answers[df_columns_names_list]\n",
                "\n",
                "index_column_agreement = df_columns_names_list.index('Agreement score')\n",
                "df_features_with_answers.insert(index_column_agreement, 'Agreement answer', agreement_answers_list)\n",
                "df_features_with_answers.insert(col_index_basicness, 'Basicness score answer', computed_answers_list)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                           Synset  Depth  Normalized Depth  isHard mean   \n",
                        "0           Synset('entity.n.01')      0          1.000000          NaN  \\\n",
                        "1  Synset('physical_entity.n.01')      1          0.947368          NaN   \n",
                        "2      Synset('abstraction.n.06')      1          0.947368          NaN   \n",
                        "3            Synset('thing.n.12')      2          0.894737          NaN   \n",
                        "4           Synset('object.n.01')      2          0.894737          NaN   \n",
                        "\n",
                        "   timeDiffs mean  Normalized timeDiffs mean  LCH Similarity   \n",
                        "0             NaN                        NaN        3.637586  \\\n",
                        "1             NaN                        NaN        2.944439   \n",
                        "2             NaN                        NaN        2.944439   \n",
                        "3             NaN                        NaN        2.538974   \n",
                        "4             NaN                        NaN        2.538974   \n",
                        "\n",
                        "   Normalized LCH Similarity  Frequency  Normalized Frequency  Synonymy   \n",
                        "0                   1.000000   0.000013              0.765746         1  \\\n",
                        "1                   0.809449   0.000011              0.661326         1   \n",
                        "2                   0.809449   0.000002              0.116022         2   \n",
                        "3                   0.697983   0.000525              1.000000         1   \n",
                        "4                   0.697983   0.000036              1.000000         2   \n",
                        "\n",
                        "   Normalized Synonymy  In CBT Basicness score answer  Basicness score   \n",
                        "0             1.000000    True                  basic         0.883604  \\\n",
                        "1             1.000000   False               advanced         0.515154   \n",
                        "2             0.962963    True               advanced         0.525257   \n",
                        "3             1.000000    True                  basic         0.990589   \n",
                        "4             0.962963    True                  basic         0.990589   \n",
                        "\n",
                        "  Agreement answer  Agreement score  \n",
                        "0             None              NaN  \n",
                        "1             None              NaN  \n",
                        "2             None              NaN  \n",
                        "3             None              NaN  \n",
                        "4             None              NaN  \n"
                    ]
                }
            ],
            "source": [
                "print(df_features_with_answers[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Salvataggio su file del dataframe aggiornato con lo score di basicness"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"\\nsorted_normalized_basicness_scores = dict(sorted(basicness_scores_dict.items(), key=lambda item: item[1], reverse=True))\\n\\nfile_name = 'basicness_scores_with_agreement.txt'\\nwith open(file_name, 'w') as file:\\n    for key, value in sorted_normalized_basicness_scores.items():\\n        file.write(str(key))\\n        file.write('\\n')\\n        file.write(str(value))\\n        file.write('\\n\\n')\\n\""
                        ]
                    },
                    "execution_count": 66,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\"\"\"\n",
                "sorted_normalized_basicness_scores = dict(sorted(basicness_scores_dict.items(), key=lambda item: item[1], reverse=True))\n",
                "\n",
                "file_name = 'basicness_scores_with_agreement.txt'\n",
                "with open(file_name, 'w') as file:\n",
                "    for key, value in sorted_normalized_basicness_scores.items():\n",
                "        file.write(str(key))\n",
                "        file.write('\\n')\n",
                "        file.write(str(value))\n",
                "        file.write('\\n\\n')\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "df_file_output = 'df_basicness_score.csv'\n",
                "df_features_with_answers.to_csv(df_file_output, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Risultati ottenuti"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                           Synset  Depth  Normalized Depth  isHard mean   \n",
                        "0           Synset('entity.n.01')      0          1.000000          NaN  \\\n",
                        "1  Synset('physical_entity.n.01')      1          0.947368          NaN   \n",
                        "2      Synset('abstraction.n.06')      1          0.947368          NaN   \n",
                        "3            Synset('thing.n.12')      2          0.894737          NaN   \n",
                        "4           Synset('object.n.01')      2          0.894737          NaN   \n",
                        "5            Synset('whole.n.02')      3          0.842105          0.0   \n",
                        "6         Synset('congener.n.03')      4          0.789474          NaN   \n",
                        "7     Synset('living_thing.n.01')      4          0.789474          NaN   \n",
                        "8         Synset('organism.n.01')      5          0.736842          NaN   \n",
                        "9          Synset('benthos.n.02')      6          0.684211          NaN   \n",
                        "\n",
                        "   timeDiffs mean  Normalized timeDiffs mean  LCH Similarity   \n",
                        "0             NaN                        NaN        3.637586  \\\n",
                        "1             NaN                        NaN        2.944439   \n",
                        "2             NaN                        NaN        2.944439   \n",
                        "3             NaN                        NaN        2.538974   \n",
                        "4             NaN                        NaN        2.538974   \n",
                        "5          1.2526                   0.000895        2.251292   \n",
                        "6             NaN                        NaN        2.028148   \n",
                        "7             NaN                        NaN        2.028148   \n",
                        "8             NaN                        NaN        1.845827   \n",
                        "9             NaN                        NaN        1.691676   \n",
                        "\n",
                        "   Normalized LCH Similarity     Frequency  Normalized Frequency  Synonymy   \n",
                        "0                   1.000000  1.320000e-05              0.765746         1  \\\n",
                        "1                   0.809449  1.140000e-05              0.661326         1   \n",
                        "2                   0.809449  2.000000e-06              0.116022         2   \n",
                        "3                   0.697983  5.250000e-04              1.000000         1   \n",
                        "4                   0.697983  3.630000e-05              1.000000         2   \n",
                        "5                   0.618897  2.880000e-04              1.000000         2   \n",
                        "6                   0.557553  4.680000e-08              0.002715         1   \n",
                        "7                   0.557553  1.450000e-04              1.000000         2   \n",
                        "8                   0.507432  3.890000e-06              0.225663         2   \n",
                        "9                   0.465055  5.130000e-08              0.002976         1   \n",
                        "\n",
                        "   Normalized Synonymy  In CBT Basicness score answer  Basicness score   \n",
                        "0             1.000000    True                  basic         0.883604  \\\n",
                        "1             1.000000   False               advanced         0.515154   \n",
                        "2             0.962963    True               advanced         0.525257   \n",
                        "3             1.000000    True                  basic         0.990589   \n",
                        "4             0.962963    True                  basic         0.990589   \n",
                        "5             0.962963    True                  basic         0.981178   \n",
                        "6             1.000000   False               advanced         0.133211   \n",
                        "7             0.962963   False               advanced         0.668807   \n",
                        "8             0.962963   False               advanced         0.243536   \n",
                        "9             1.000000   False               advanced         0.114530   \n",
                        "\n",
                        "  Agreement answer  Agreement score  \n",
                        "0             None              NaN  \n",
                        "1             None              NaN  \n",
                        "2             None              NaN  \n",
                        "3             None              NaN  \n",
                        "4             None              NaN  \n",
                        "5            basic              1.0  \n",
                        "6             None              NaN  \n",
                        "7             None              NaN  \n",
                        "8             None              NaN  \n",
                        "9             None              NaN  \n"
                    ]
                }
            ],
            "source": [
                "print(df_features_with_answers[:10])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Numero di synset classificati come 'ADVANCED' e come 'BASIC'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "I synset classificati come 'ADVANCED' sono: 74394\n",
                        "I primi 10 synset classificati come 'ADVANCED' sono:                                 0\n",
                        "0  Synset('physical_entity.n.01')\n",
                        "1      Synset('abstraction.n.06')\n",
                        "2         Synset('congener.n.03')\n",
                        "3     Synset('living_thing.n.01')\n",
                        "4         Synset('organism.n.01')\n",
                        "5          Synset('benthos.n.02')\n",
                        "6            Synset('dwarf.n.03')\n",
                        "7      Synset('heterotroph.n.01')\n",
                        "8            Synset('biont.n.01')\n",
                        "9     Synset('causal_agent.n.01')\n",
                        "\n",
                        "I synset classificati come 'BASIC' sono: 7721\n",
                        "I primi 10 synset classificati come 'BASIC' sono:                        0\n",
                        "0  Synset('entity.n.01')\n",
                        "1   Synset('thing.n.12')\n",
                        "2  Synset('object.n.01')\n",
                        "3   Synset('whole.n.02')\n",
                        "4  Synset('parent.n.02')\n",
                        "5    Synset('life.n.10')\n",
                        "6    Synset('cell.n.02')\n",
                        "7  Synset('person.n.01')\n",
                        "8  Synset('animal.n.01')\n",
                        "9   Synset('plant.n.02')\n"
                    ]
                }
            ],
            "source": [
                "def count_basic_advanced(dataframe):\n",
                "    count_advanced = 0\n",
                "    count_basic = 0\n",
                "    basic_synset_list, advanced_synset_list = [], []\n",
                "    for val, synset in zip(dataframe['Basicness score answer'], dataframe['Synset']):\n",
                "        if val == 'advanced':\n",
                "            count_advanced += 1\n",
                "            advanced_synset_list.append(synset)\n",
                "        else:\n",
                "            count_basic += 1\n",
                "            basic_synset_list.append(synset)\n",
                "    \n",
                "    return count_advanced, count_basic, advanced_synset_list, basic_synset_list\n",
                "\n",
                "num_advanced_syn, num_basic_syn, advanced_synset_list, basic_synset_list = count_basic_advanced(df_features_with_answers)\n",
                "\n",
                "print(\"I synset classificati come 'ADVANCED' sono:\", num_advanced_syn)\n",
                "print(\"I primi 10 synset classificati come 'ADVANCED' sono:\", pd.DataFrame(advanced_synset_list)[:10])\n",
                "\n",
                "print(\"\\nI synset classificati come 'BASIC' sono:\", num_basic_syn)\n",
                "print(\"I primi 10 synset classificati come 'BASIC' sono:\", pd.DataFrame(basic_synset_list)[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Il numero di synset annotati nel gold standard è: 504\n"
                    ]
                }
            ],
            "source": [
                "valori_non_vuoti = df_features_with_answers['Agreement score'].notnull()\n",
                "count_syn_with_agreement = valori_non_vuoti.sum()\n",
                "print('Il numero di synset annotati nel gold standard è:', count_syn_with_agreement)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Numero di synset non classificati come nel gold standard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Il numero di synset non classificati come nel gold standard è: 37\n",
                        "                              Basicness score  Agreement score   \n",
                        "Synset('motivation.n.01')                 0.9         0.658338  \\\n",
                        "Synset('means.n.01')                      1.0         0.649985   \n",
                        "Synset('inversion.n.08')                  0.7         0.150812   \n",
                        "Synset('behalf.n.01')                     0.0         0.915301   \n",
                        "Synset('contribution.n.01')               0.3         0.943534   \n",
                        "Synset('workshop.n.02')                   0.5         0.836356   \n",
                        "Synset('homo.n.02')                       0.7         0.173856   \n",
                        "Synset('avenue.n.02')                     0.4         0.924712   \n",
                        "Synset('can.n.01')                        1.0         0.640574   \n",
                        "Synset('fabric.n.01')                     0.5         0.799163   \n",
                        "Synset('determination.n.02')              0.6         0.877051   \n",
                        "Synset('human_body.n.01')                 0.8         0.649985   \n",
                        "Synset('convention.n.02')                 0.4         0.962356   \n",
                        "Synset('estimate.n.01')                   0.6         0.924712   \n",
                        "Synset('real_world.n.01')                 0.8         0.640574   \n",
                        "Synset('texture.n.02')                    0.8         0.652789   \n",
                        "Synset('reputation.n.03')                 0.6         0.943534   \n",
                        "Synset('heading.n.01')                    0.3         0.934123   \n",
                        "Synset('artwork.n.01')                    0.7         0.533653   \n",
                        "Synset('meter.n.03')                      0.9         0.477317   \n",
                        "Synset('hearing.n.05')                    0.5         0.934123   \n",
                        "Synset('examination.n.02')                0.3         0.952945   \n",
                        "Synset('businessmen.n.01')                0.7         0.519574   \n",
                        "Synset('committee.n.01')                  0.4         0.943534   \n",
                        "Synset('domain.n.02')                     0.6         0.962356   \n",
                        "Synset('peak.n.04')                       0.6         0.952945   \n",
                        "Synset('narrow.n.01')                     0.5         0.952945   \n",
                        "Synset('administrator.n.01')              0.6         0.780406   \n",
                        "Synset('ally.n.02')                       0.3         0.808446   \n",
                        "Synset('buddy.n.01')                      0.6         0.962356   \n",
                        "Synset('chancellor.n.03')                 0.0         0.817663   \n",
                        "Synset('male_child.n.01')                 0.9         0.659396   \n",
                        "Synset('provision.n.04')                  0.1         0.789366   \n",
                        "Synset('branch.n.03')                     0.6         0.952945   \n",
                        "Synset('sphere.n.01')                     0.8         0.669182   \n",
                        "Synset('bedtime.n.01')                    0.8         0.500752   \n",
                        "Synset('snap.n.02')                       0.4         0.911193   \n",
                        "\n",
                        "                              timeDiffs mean  isHard mean  \n",
                        "Synset('motivation.n.01')             1.6117          0.0  \n",
                        "Synset('means.n.01')                  1.6045          0.0  \n",
                        "Synset('inversion.n.08')              4.3690          0.0  \n",
                        "Synset('behalf.n.01')                 8.2441          0.0  \n",
                        "Synset('contribution.n.01')           2.9302          0.1  \n",
                        "Synset('workshop.n.02')               2.4265          0.1  \n",
                        "Synset('homo.n.02')                   2.2972          0.1  \n",
                        "Synset('avenue.n.02')                 5.3366          0.3  \n",
                        "Synset('can.n.01')                    1.1273          0.0  \n",
                        "Synset('fabric.n.01')                 6.2809          0.1  \n",
                        "Synset('determination.n.02')          1.8504          0.0  \n",
                        "Synset('human_body.n.01')             2.6120          0.2  \n",
                        "Synset('convention.n.02')            44.4339          0.0  \n",
                        "Synset('estimate.n.01')               4.6611          0.0  \n",
                        "Synset('real_world.n.01')             2.1319          0.1  \n",
                        "Synset('texture.n.02')                4.0939          0.1  \n",
                        "Synset('reputation.n.03')             2.0320          0.0  \n",
                        "Synset('heading.n.01')                3.8707          0.0  \n",
                        "Synset('artwork.n.01')                1.3147          0.0  \n",
                        "Synset('meter.n.03')                  4.3705          0.1  \n",
                        "Synset('hearing.n.05')                3.5528          0.0  \n",
                        "Synset('examination.n.02')            2.5411          0.0  \n",
                        "Synset('businessmen.n.01')            2.1302          0.1  \n",
                        "Synset('committee.n.01')              2.4044          0.1  \n",
                        "Synset('domain.n.02')                 1.8479          0.0  \n",
                        "Synset('peak.n.04')                   8.6684          0.1  \n",
                        "Synset('narrow.n.01')                 4.0557          0.2  \n",
                        "Synset('administrator.n.01')          2.9636          0.1  \n",
                        "Synset('ally.n.02')                   5.9401          0.1  \n",
                        "Synset('buddy.n.01')                  1.2855          0.0  \n",
                        "Synset('chancellor.n.03')             2.3450          0.1  \n",
                        "Synset('male_child.n.01')             3.0224          0.0  \n",
                        "Synset('provision.n.04')              1.5380          0.0  \n",
                        "Synset('branch.n.03')                 1.3731          0.0  \n",
                        "Synset('sphere.n.01')                 2.2203          0.1  \n",
                        "Synset('bedtime.n.01')                2.5368          0.0  \n",
                        "Synset('snap.n.02')                   2.8482          0.0  \n"
                    ]
                }
            ],
            "source": [
                "def get_syn_with_wrong_answer(dataframe):\n",
                "    syn_with_wrong_answer_dict = {}\n",
                "    for _, riga in dataframe.iterrows():\n",
                "        answer_agreement = riga['Agreement answer']\n",
                "        answer_score = riga['Basicness score answer']\n",
                "        if pd.isna(answer_agreement):\n",
                "            pass\n",
                "        elif answer_agreement != answer_score:\n",
                "            synset = riga['Synset']\n",
                "            agreem_score = riga['Agreement score']\n",
                "            basicness_score = riga['Basicness score']\n",
                "            time_diff = riga['timeDiffs mean']\n",
                "            is_hard_mean = riga['isHard mean']\n",
                "            syn_with_wrong_answer_dict[synset] = (agreem_score, basicness_score, time_diff, is_hard_mean)\n",
                "    return syn_with_wrong_answer_dict\n",
                "\n",
                "syn_with_wrong_answer_dict = get_syn_with_wrong_answer(df_features_with_answers)\n",
                "print(\"Il numero di synset non classificati come nel gold standard è:\", len(syn_with_wrong_answer_dict))\n",
                "df_syn_with_wrong_answer = pd.DataFrame(list(syn_with_wrong_answer_dict.values()), \n",
                "                                        index=syn_with_wrong_answer_dict.keys(), \n",
                "                                        columns=['Basicness score', 'Agreement score', 'timeDiffs mean', 'isHard mean'])\n",
                "print(df_syn_with_wrong_answer)\n",
                "\n",
                "# salvataggio su file\n",
                "df_syn_with_wrong_answer.insert(0, 'Synset', df_syn_with_wrong_answer.index)\n",
                "df_file_output = 'df_syn_with_wrong_answer.csv'\n",
                "df_syn_with_wrong_answer.to_csv(df_file_output, index=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Numero di synset non classificati come nel gold standard che sono HARD."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Il numero di synset considerati 'hard' (con isHard mean >= 0.6990000000000003) è: 0\n",
                        "Lista di synset considerati 'hard':\n",
                        "[]\n"
                    ]
                }
            ],
            "source": [
                "def find_hard_synsets(syn_with_wrong_answer_dict, threshold):\n",
                "    hard_synsets = []\n",
                "    for synset, values in syn_with_wrong_answer_dict.items():\n",
                "        is_hard_mean = values[3]\n",
                "\n",
                "        if is_hard_mean >= threshold:\n",
                "            hard_synsets.append(synset)\n",
                "\n",
                "    return hard_synsets\n",
                "\n",
                "def count_hard_synsets(syn_with_wrong_answer_dict, threshold):\n",
                "    hard_synsets = find_hard_synsets(syn_with_wrong_answer_dict, threshold)\n",
                "    return len(hard_synsets), hard_synsets\n",
                "\n",
                "num_hard_synsets, hard_synsets_list = count_hard_synsets(syn_with_wrong_answer_dict, threshold)\n",
                "print(f\"Il numero di synset considerati 'hard' (con isHard mean >= {threshold}) è:\", num_hard_synsets)\n",
                "print(\"Lista di synset considerati 'hard':\")\n",
                "print(hard_synsets_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Valori di isHard mean e timeDiffs mean di un synset specifico"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Il valore di 'isHard mean' per il 'Synset('motivation.n.01')' è: 0.0\n",
                        "Il Synset('motivation.n.01') è stato quindi valutato come NON HARD in base alla soglia 0.6990000000000003\n",
                        "\n",
                        "Il valore di 'timeDiffs mean' per il synset 'Synset('motivation.n.01')' è: 1.6116999999999997\n"
                    ]
                }
            ],
            "source": [
                "def get_synset_difficulty(syn_with_wrong_answer_dict, synset):\n",
                "    if synset in syn_with_wrong_answer_dict:\n",
                "        values = syn_with_wrong_answer_dict[synset]\n",
                "        is_hard_mean = values[3]\n",
                "        time_diff_mean = values[2]\n",
                "        return is_hard_mean, time_diff_mean\n",
                "    else:\n",
                "        return None, None\n",
                "\n",
                "synset = \"Synset('motivation.n.01')\"\n",
                "is_hard_mean_value, time_diff_mean_value = get_synset_difficulty(syn_with_wrong_answer_dict, synset)\n",
                "\n",
                "if is_hard_mean_value is not None and time_diff_mean_value is not None:\n",
                "    print(f\"Il valore di 'isHard mean' per il '{synset}' è:\", is_hard_mean_value)\n",
                "    print(f\"Il {synset} è stato quindi valutato come {'HARD' if is_hard_mean_value >= threshold else 'NON HARD'} in base alla soglia {threshold}\")\n",
                "    print(f\"\\nIl valore di 'timeDiffs mean' per il synset '{synset}' è:\", time_diff_mean_value)\n",
                "else:\n",
                "    print(f\"Il {synset} non è un synset con agreement umano.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **STATISTICHE**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PESI normalizzati DELLE FEATURES con only_synset_with_agreement = **False**\n",
                "\n",
                "- models[0]\n",
                "    - In CB corpus: 0.20055614247529385\n",
                "    - Normalized Depth: 0.0\n",
                "    - Normalized LCH Similarity: 0.4542789375493259\n",
                "    - Normalized Frequency: 0.23024920502670743\n",
                "    - Normalized Polysemy: 0.11491571494867293\n",
                "\n",
                "- models[1]\n",
                "    - In CB corpus: 0.19824315027636746\n",
                "    - Normalized Depth: 0.0\n",
                "    - Normalized LCH Similarity: 0.4419459557938856\n",
                "    - Normalized Frequency: 0.23795853172772272\n",
                "    - Normalized Polysemy: 0.12185236220202411\n",
                "\n",
                "- models[2]\n",
                "    - In CB corpus: 0.20069157849061653\n",
                "    - Normalized Depth: 0.0\n",
                "    - Normalized LCH Similarity: 0.4543585753964545\n",
                "    - Normalized Frequency: 0.23043251993560798\n",
                "    - Normalized Polysemy: 0.11451732617732088\n",
                "\n",
                "- models[3] aplha 0.01\n",
                "    - In CB corpus: 0.3793134122963464\n",
                "    - Normalized Depth: 0.0\n",
                "    - Normalized LCH Similarity: 0.0\n",
                "    - Normalized Frequency: 0.6206865877036536\n",
                "    - Normalized Polysemy: 0.0\n",
                "\n",
                "- models[3] aplha 0.001\n",
                "    - In CB corpus: 0.2958827130997813\n",
                "    - Normalized Depth: 0.1995377800150995\n",
                "    - Normalized LCH Similarity: 0.05309652386011924\n",
                "    - Normalized Frequency: 0.451482983025\n",
                "    - Normalized Polysemy: 0.0\n",
                "\n",
                "- models[4]\n",
                "    - In CB corpus: 0.29736353722288317\n",
                "    - Normalized Depth: 0.1755048982441552\n",
                "    - Normalized LCH Similarity: 0.0\n",
                "    - Normalized Frequency: 0.5271315645329616\n",
                "    - Normalized Polysemy: 0.0"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
