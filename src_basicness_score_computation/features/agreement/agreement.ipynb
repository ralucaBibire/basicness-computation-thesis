{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOLD STANDARD - HUMAN AGREEMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/raky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from commons_init import SYNSET_DA_EVITARE, SUPPORTED_POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione e processing dataset\n",
    "1. Dato un file .json, ricaviamo il dizionario associato.\n",
    "2. Splittiamo il campo dict['dataset'] in tre campi: 'synset', 'term', 'definition'.\n",
    "3. All'interno del dizionario sanitizziamo i campi categoriali:\n",
    "    - Campo 'synset': Synset('war.n.1') --> \"war.n.1\"\n",
    "    - Campo 'termine': 'war, warfare' --> ['war', 'warfare']\n",
    "    - Campo 'definition': tokenizzazione e rimozione di stop-words e punteggiatura\n",
    "4. Costruiamo un DataFrame con il dizionario cosí ottenuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = \"./gold_standard\"\n",
    "\n",
    "def get_json_dictionary(folder_path, filename):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    f =  open(file_path, \"r\")\n",
    "    dictionary = json.load(f)\n",
    "    f.close()\n",
    "    return dictionary\n",
    "   \n",
    "def filter_dictionary(dictionary):\n",
    "    synsets, terms, definitions = [], [], []\n",
    "\n",
    "    for entry in dictionary['dataset']:\n",
    "        entry = entry.split(':')\n",
    "        synsets.append(entry[0][8:-2])\n",
    "        terms.append(word_tokenize(re.sub(',', '', entry[1][:-12].strip())))\n",
    "        definition = ''.join(char for char in entry[2] if char not in string.punctuation)\n",
    "        definition = [word for word in word_tokenize(definition) if word not in stop_words]\n",
    "        definitions.append(definition)\n",
    "    \n",
    "    for key in ['i','date','dataset']:\n",
    "        dictionary.pop(key)\n",
    "    \n",
    "    dictionary[\"synset\"] = synsets\n",
    "    dictionary[\"term\"] = terms\n",
    "    dictionary[\"definition\"] = definitions\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def get_dataframe(folder_path, filename):\n",
    "    df = pd.DataFrame(filter_dictionary(get_json_dictionary(folder_path, filename)))\n",
    "    return df\n",
    "\n",
    "def get_all_dataframes(folder_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):    \n",
    "            dataframes.append(get_dataframe(folder_path, filename))\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agreement tra annotatori\n",
    "L'agreement viene calcolato per ogni synset andando a prendere per ogni annotatore come questo lo ha classificato. Andiamo quindi a contare quante volte il synset è stato annotato come 'basic' e dividiamo rispetto al numero totale di annotatori.  \n",
    "Per esempio \"war\" è stato classificato da tutti gli annotatori come \"basic\" dunque l'agreement score sarà 10/10 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement_mean(folder_path):\n",
    "    lista_dataframe = get_all_dataframes(folder_path)\n",
    "    # print(lista_dataframe[0][:10])\n",
    "\n",
    "    ordine_colonne = lista_dataframe[0].columns\n",
    "    nuova_lista_dataframe = [df.reindex(columns=ordine_colonne) for df in lista_dataframe]\n",
    "    ordine_colonne = nuova_lista_dataframe[0].columns\n",
    "    \n",
    "    agreement_scores_dict, isHard_dict, timeDiffs_dict = {}, {}, {}\n",
    "    for df in nuova_lista_dataframe:\n",
    "        for colonna in df.columns:\n",
    "            data = pd.DataFrame({f'df{i+1}': df[colonna] for i, df in enumerate(nuova_lista_dataframe)})\n",
    "            if colonna == 'answers':\n",
    "                data['agreement'] = data.eq('basic').mean(axis=1)\n",
    "                agreement_scores_dict = {synset: score for synset, score in zip(nuova_lista_dataframe[0]['synset'], data['agreement'])}\n",
    "                agreements_list = data['agreement'].tolist()\n",
    "            elif colonna == 'isHard':\n",
    "                data['isHard'] = data.eq(True).mean(axis=1)\n",
    "                isHard_dict = {synset: answ for synset, answ in zip(nuova_lista_dataframe[0]['synset'], data['isHard'])}\n",
    "                isHard_list = data['isHard'].tolist()\n",
    "            elif colonna == 'timeDiffs':\n",
    "                data['timeDiffs'] = data.mean(axis=1)\n",
    "                timeDiffs_dict = {synset: answ for synset, answ in zip(nuova_lista_dataframe[0]['synset'], data['timeDiffs'])}\n",
    "                timeDiffs_list = data['timeDiffs'].tolist()\n",
    "                #normalizzazione\n",
    "                max_timeDiffs = max(data['timeDiffs'])\n",
    "                min_timeDiffs = 0.0\n",
    "                timeDiffs_dict_norm = {synset: (answ - min_timeDiffs) / (max_timeDiffs - min_timeDiffs) for synset, answ in zip(nuova_lista_dataframe[0]['synset'], data['timeDiffs'])}\n",
    "                \n",
    "    return agreements_list, agreement_scores_dict, isHard_list, isHard_dict, timeDiffs_list, timeDiffs_dict, timeDiffs_dict_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'war.n.01': 1.0, 'fiefdom.n.01': 0.0, 'bed.n.03': 1.0, 'return_on_invested_capital.n.01': 0.0, 'texture.n.02': 0.8, 'news.n.01': 1.0, 'look.n.02': 1.0, 'caddy.n.01': 0.2, 'weeder.n.01': 0.0, 'avenue.n.02': 0.4, 'adar.n.01': 0.0, 'bedtime.n.01': 0.8, 'inversion.n.08': 0.7, 'yak.n.01': 0.1, 'breath.n.05': 0.9, 'executive_clemency.n.01': 0.0, 'muse.n.02': 0.3, 'effect.n.06': 1.0, 'quickening.n.02': 0.1, 'sleeper.n.09': 0.5, 'caravanning.n.01': 0.0, 'jotter.n.01': 0.0, 'armageddon.n.02': 0.2, 'compass_point.n.01': 0.1, 'blackwater_fever.n.01': 0.0, 'respect.n.03': 1.0, 'position.n.06': 1.0, 'message.n.02': 1.0, 'arrest.n.02': 0.8, 'motivation.n.01': 0.9, 'day.n.04': 1.0, 'nose_cone.n.01': 0.0, 'discussion.n.02': 0.9, 'glow.n.05': 0.5, 'alcalde.n.01': 0.0, 'draft_board.n.01': 0.2, 'multitude.n.03': 0.4, 'hour.n.02': 1.0, 'book.n.02': 1.0, 'degree.n.02': 1.0, 'show-stopper.n.01': 0.0, 'military_position.n.01': 0.2, 'top.n.09': 1.0, 'pillar_of_islam.n.01': 0.1, 'power.n.05': 1.0, 'dail_eireann.n.01': 0.0, 'record.n.05': 1.0, 'pace.n.03': 0.7, 'american_labor_party.n.01': 0.0, 'nook_and_cranny.n.01': 0.0, \"lady's_smock.n.01\": 0.0, 'obstacle_race.n.01': 0.2, 'clasp.n.02': 0.0, 'stuff.n.07': 1.0, 'succubus.n.01': 0.1, 'form.n.03': 1.0, 'quantity.n.02': 1.0, 'commercial_enterprise.n.02': 0.3, 'distaff.n.01': 0.1, 'company.n.01': 1.0, 'hammerlock.n.01': 0.0, 'fact.n.01': 1.0, 'niche.n.01': 0.0, 'pinnacle.n.03': 0.1, 'function.n.03': 0.9, 'male_child.n.01': 0.9, 'direction.n.06': 0.9, 'violence.n.01': 1.0, 'shape.n.01': 1.0, 'movie.n.01': 1.0, 'impudence.n.01': 0.2, 'movement.n.04': 1.0, 'argument.n.05': 0.9, 'esteem.n.01': 0.1, 'viewing_audience.n.01': 0.2, 'incipiency.n.01': 0.0, 'fire.n.02': 0.9, 'handle.n.01': 0.8, 'course.n.01': 1.0, 'latest.n.01': 0.9, 'means.n.01': 1.0, 'rate.n.01': 0.9, 'monetary_value.n.01': 0.2, 'dwelling.n.01': 0.0, 'test.n.05': 1.0, 'peroration.n.02': 0.0, 'light.n.01': 1.0, 'accompaniment.n.02': 0.1, 'fall.n.06': 1.0, 'rainbow.n.02': 0.9, 'stationary_stochastic_process.n.01': 0.0, 'assessment.n.03': 0.4, 'decision.n.01': 1.0, 'homo_sapiens.n.01': 0.4, 'management.n.01': 0.9, 'touch.n.10': 1.0, 'pommel.n.01': 0.0, 'blood_brother.n.02': 0.3, 'homo.n.02': 0.7, 'say.n.01': 1.0, 'descant.n.01': 0.1, 'increase.n.03': 0.8, 'board.n.01': 1.0, 'rescript.n.01': 0.1, 'reaction.n.03': 0.9, 'exploitation.n.01': 0.2, 'weather.n.01': 1.0, 'schutzstaffel.n.01': 0.0, 'antilogarithm.n.01': 0.0, 'helpmate.n.01': 0.3, 'bill.n.03': 1.0, 'hegemon.n.01': 0.0, 'word.n.01': 1.0, 'real_world.n.01': 0.8, 'car.n.01': 1.0, 'sign.n.01': 1.0, 'handwriting.n.01': 0.6, 'hit.n.03': 1.0, 'boucle.n.01': 0.0, 'doggedness.n.01': 0.0, 'attempt.n.01': 0.8, 'town.n.01': 1.0, 'aid.n.02': 0.8, 'conservatory.n.02': 0.4, 'art.n.03': 1.0, 'toilet.n.01': 1.0, 'year.n.01': 1.0, 'process.n.05': 0.9, 'hope.n.02': 1.0, 'line.n.29': 1.0, 'base.n.08': 0.9, 'speech.n.02': 1.0, 'basis.n.02': 0.9, 'lineage.n.01': 0.0, 'afternoon.n.01': 1.0, 'colonialist.n.01': 0.2, 'poverty_trap.n.01': 0.1, 'thinking.n.01': 0.9, 'affray.n.02': 0.1, 'demonstrative_pronoun.n.01': 0.1, \"ladies'_room.n.01\": 0.5, 'burg.n.01': 0.0, 'higher_law.n.01': 0.1, 'doubleton.n.01': 0.0, 'woman.n.01': 1.0, 'fight.n.02': 1.0, 'playing_field.n.02': 0.6, 'road_rage.n.01': 0.0, 'cusp.n.01': 0.0, 'guess.n.02': 0.9, 'businessmen.n.01': 0.7, 'strait.n.01': 0.2, 'narrow.n.01': 0.5, 'left.n.02': 1.0, 'sound.n.01': 1.0, 'military_action.n.01': 0.4, 'second_nature.n.01': 0.3, 'friend.n.01': 1.0, 'branch.n.03': 0.6, 'outtake.n.01': 0.1, 'edition.n.03': 0.9, 'card.n.08': 1.0, 'gazette.n.01': 0.1, 'penny_ante.n.01': 0.0, 'double_blind.n.01': 0.0, 'mainstream.n.01': 0.5, 'shooting.n.01': 0.7, 'cantle.n.01': 0.0, 'countenance.n.03': 0.0, 'strand.n.01': 0.0, 'stature.n.01': 0.3, 'meaning.n.02': 1.0, 'glissade.n.01': 0.0, 'footrace.n.01': 0.0, 'deal.n.01': 0.9, 'room.n.01': 1.0, 'school.n.02': 1.0, 'sum.n.01': 1.0, 'circumstance.n.03': 0.5, 'stripe.n.04': 0.4, 'link.n.06': 1.0, 'predetermination.n.02': 0.1, 'kin.n.01': 0.2, 'relative.n.01': 1.0, 'mafia.n.01': 0.4, 'child.n.01': 1.0, 'eastern_church.n.02': 0.0, 'chine.n.02': 0.1, 'pop_quiz.n.01': 0.4, 'shiva.n.01': 0.0, 'bonding.n.02': 0.2, 'artwork.n.01': 0.7, 'start.n.01': 1.0, 'probability.n.01': 0.8, 'meat_and_potatoes.n.01': 0.6, 'part.n.09': 1.0, 'myringotomy.n.01': 0.0, 'south_sea_islands.n.01': 0.2, 'golden_age.n.01': 0.3, 'broadcast_journalist.n.01': 0.2, 'issue.n.02': 0.9, 'universe.n.01': 1.0, 'anagnost.n.01': 0.0, 'strange_attractor.n.01': 0.0, 'token.n.02': 0.6, 'supporter.n.01': 0.7, 'plot_element.n.01': 0.0, 'syndicate.n.01': 0.1, 'death.n.04': 1.0, 'contingent.n.02': 0.2, 'thought.n.03': 1.0, 'jihad.n.01': 0.3, 'royal_road.n.01': 0.3, 'theme.n.03': 0.8, 'oblateness.n.01': 0.0, 'estimate.n.01': 0.6, 'nation.n.02': 1.0, 'sphere.n.01': 0.8, 'advertence.n.01': 0.2, 'holy_order.n.01': 0.1, 'judgment.n.01': 0.9, 'air.n.03': 1.0, 'party.n.01': 1.0, 'running.n.03': 0.8, 'week.n.01': 1.0, 'front.n.04': 0.9, 'discovery.n.01': 0.9, 'counterbombardment.n.01': 0.0, 'business.n.01': 0.9, 'meeting.n.03': 1.0, 'world_power.n.01': 0.2, 'side.n.04': 1.0, 'vitamin_pill.n.01': 0.4, 'shadow_cabinet.n.01': 0.0, 'incandescence.n.02': 0.0, 'undivided_interest.n.01': 0.0, 'enfilade.n.01': 0.0, 'support_level.n.01': 0.2, 'newspaper.n.01': 1.0, 'fresco.n.01': 0.4, 'logjam.n.01': 0.0, 'peeper.n.02': 0.0, 'trinity.n.02': 0.2, 'plea_bargain.n.01': 0.0, 'urgency.n.01': 0.5, 'writing.n.03': 1.0, 'tempestuousness.n.01': 0.0, 'daub.n.01': 0.0, 'sawhorse.n.01': 0.0, 'greece.n.02': 0.5, 'system.n.02': 0.9, 'slush.n.01': 0.0, 'amount.n.02': 0.9, 'art_nouveau.n.01': 0.0, 'fun.n.02': 1.0, 'nature_study.n.01': 0.2, 'act.n.04': 1.0, 'auspices.n.01': 0.0, 'ability.n.02': 1.0, 'hose.n.03': 0.3, 'opinion.n.04': 1.0, 'safety_match.n.01': 0.2, 'point.n.01': 1.0, 'rule.n.11': 1.0, 'committee.n.01': 0.4, 'founder.n.02': 0.8, 'eye-beaming.n.01': 0.0, 'sprachgefuhl.n.01': 0.0, 'unitization.n.04': 0.0, 'fabric.n.01': 0.5, 'silver_certificate.n.01': 0.0, 'luminosity.n.01': 0.5, 'wall.n.01': 1.0, 'dynamism.n.03': 0.2, 'earning_per_share.n.01': 0.0, 'subsistence.n.01': 0.0, 'convention.n.02': 0.4, 'obiter_dictum.n.02': 0.0, 'experience.n.02': 1.0, 'hosanna.n.01': 0.0, 'deposition.n.01': 0.1, 'looseness.n.05': 0.1, 'end.n.11': 1.0, 'money.n.01': 1.0, 'administrator.n.01': 0.6, 'model_t.n.01': 0.0, 'procedure.n.01': 0.8, 'margin.n.05': 0.7, 'infantry.n.01': 0.2, 'store.n.02': 1.0, 'particular.n.01': 1.0, 'price.n.02': 1.0, 'paratroops.n.01': 0.0, 'sanguinity.n.01': 0.0, 'meritocracy.n.01': 0.1, 'conjunction.n.05': 0.2, 'education.n.01': 0.9, 'bilge.n.02': 0.0, 'chancellor.n.03': 0.0, 'music.n.01': 1.0, 'meter.n.03': 0.9, 'associateship.n.01': 0.0, 'behalf.n.01': 0.0, 'protocol.n.03': 0.5, 'cry.n.01': 1.0, 'painting.n.01': 1.0, 'couple.n.04': 1.0, 'flower.n.03': 1.0, 'designation.n.03': 0.0, 'cutaneous_sensation.n.01': 0.0, 'support.n.06': 0.9, 'fork.n.03': 1.0, 'motion.n.03': 0.8, 'thing.n.04': 1.0, 'ground.n.05': 0.8, 'athenaeum.n.01': 0.0, 'juggernaut.n.01': 0.0, 'office.n.04': 1.0, 'work_force.n.01': 0.4, 'exemplar.n.01': 0.4, 'holy_year.n.01': 0.1, 'numeral.n.01': 0.6, 'brownie.n.01': 0.3, 'shrillness.n.01': 0.0, 'mire.n.01': 0.0, 'hypothesis.n.02': 0.3, 'arboriculture.n.01': 0.0, 'sea-duty.n.01': 0.0, 'peak.n.04': 0.6, 'police.n.01': 1.0, 'heading.n.01': 0.3, 'earth.n.02': 1.0, 'thelarche.n.01': 0.0, 'feeling.n.06': 1.0, 'source.n.03': 0.9, 'manipulation.n.01': 0.2, 'toleration.n.02': 0.1, 'puppetry.n.01': 0.0, 'condition.n.01': 0.8, 'water.n.01': 1.0, 'scene.n.04': 1.0, 'time.n.03': 1.0, 'human_body.n.01': 0.8, 'country.n.02': 1.0, 'palestra.n.01': 0.4, 'promise.n.02': 0.8, 'quality.n.03': 0.9, 'report.n.01': 0.9, 'hermitage.n.01': 0.2, 'title.n.01': 1.0, 'street.n.01': 1.0, 'value.n.01': 1.0, 'table.n.01': 1.0, 'plot.n.01': 1.0, 'eye.n.01': 1.0, 'field.n.01': 1.0, 'examination.n.02': 0.3, 'mind_game.n.02': 0.2, 'step.n.03': 1.0, 'habit.n.02': 0.9, 'activity.n.01': 1.0, 'thousand.n.01': 1.0, 'brass_monkey.n.01': 0.0, 'use.n.01': 1.0, 'lighter.n.02': 0.8, 'head.n.04': 1.0, 'soubrette.n.02': 0.0, 'kind.n.01': 1.0, 'justification.n.01': 0.4, 'early-morning_hour.n.01': 0.0, 'calendar.n.03': 1.0, 'calligraphy.n.01': 0.1, 'change.n.01': 1.0, 'magazine.n.03': 1.0, 'spinal_column.n.01': 0.2, 'need.n.01': 0.9, 'face.n.01': 1.0, 'reputation.n.03': 0.6, 'activeness.n.02': 0.0, 'determination.n.02': 0.6, 'spaghetti_western.n.01': 0.0, 'natural_process.n.01': 0.3, 'qing.n.01': 0.0, 'art_class.n.01': 0.5, 'military_unit.n.01': 0.3, 'ruse.n.01': 0.0, 'colonizer.n.01': 0.1, 'bell_gable.n.01': 0.0, 'place.n.10': 1.0, 'voice.n.01': 1.0, 'jocosity.n.01': 0.0, 'interest.n.03': 1.0, 'land.n.02': 1.0, 'calendar_month.n.01': 0.5, 'cross_section.n.03': 0.1, 'union_shop.n.01': 0.1, 'hairy_tongue.n.01': 0.0, 'libidinal_energy.n.01': 0.0, 'answer.n.01': 1.0, 'velleity.n.01': 0.0, 'club.n.02': 1.0, 'whole.n.02': 1.0, 'spirit.n.04': 1.0, 'attention.n.01': 1.0, 'swiss.n.01': 0.5, 'region.n.01': 1.0, 'godhead.n.01': 0.0, 'can.n.01': 1.0, 'assay.n.03': 0.1, 'space.n.07': 1.0, 'judge.n.01': 0.9, 'crest.n.05': 0.1, 'contribution.n.01': 0.3, 'secondo.n.01': 0.2, 'contractor.n.02': 0.1, 'wish.n.01': 1.0, 'ally.n.02': 0.3, 'pinpoint.n.01': 0.0, 'workshop.n.02': 0.5, 'side_yard.n.01': 0.1, 'grave.n.01': 0.7, 'southwest_by_west.n.01': 0.0, 'multitudinousness.n.01': 0.0, 'church.n.01': 1.0, 'post.n.01': 1.0, 'detail.n.01': 1.0, 'workplace.n.01': 0.6, 'bridge_player.n.01': 0.0, 'kernel.n.03': 0.0, 'white.n.11': 1.0, 'illustration.n.01': 0.4, 'vibration.n.04': 0.4, 'anapest.n.01': 0.0, 'opportunity.n.01': 0.7, 'meteortropism.n.01': 0.0, 'rest.n.02': 1.0, 'bridgehead.n.02': 0.0, 'island.n.01': 1.0, \"ship's_company.n.01\": 0.0, 'dance_step.n.01': 0.4, 'writer.n.01': 1.0, 'promptness.n.01': 0.0, 'domain.n.02': 0.6, 'buddy.n.01': 0.6, 'care.n.01': 1.0, 'instruction.n.04': 0.9, 'continental_shelf.n.01': 0.0, 'character.n.04': 0.9, 'hearing.n.05': 0.5, 'back.n.08': 1.0, 'trip.n.06': 1.0, 'recording_studio.n.01': 0.2, 'person.n.02': 1.0, 'koinonia.n.01': 0.0, 'undertone.n.02': 0.2, 'principal.n.02': 0.8, 'conclusion.n.08': 0.8, 'action.n.02': 1.0, 'hired_hand.n.01': 0.0, 'viewgraph.n.01': 0.0, 'play.n.06': 1.0, 'house.n.05': 1.0, 'mull.n.01': 0.1, 'prodigy.n.03': 0.1, 'beginning.n.02': 0.9, 'breeze.n.01': 0.2, 'rubicon.n.02': 0.0, 'set.n.05': 1.0, 'oeuvre.n.01': 0.1, 'snap.n.02': 0.4, 'cocktail_lounge.n.01': 0.0, 'autolysis.n.01': 0.0, 'body.n.02': 1.0, 'terror.n.03': 0.9, 'moment.n.01': 1.0, 'diminution.n.02': 0.1, 'trestle.n.02': 0.0, 'situation.n.02': 0.9, 'subsidization.n.01': 0.0, 'provision.n.04': 0.1, 'lateral_thinking.n.01': 0.0, 'intrigue.n.01': 0.0, 'talk.n.01': 1.0, 'way.n.06': 1.0, 'gibson_girl.n.01': 0.0, 'family.n.08': 1.0, 'millenary.n.03': 0.3, 'desensitization_technique.n.01': 0.0, 'natural_order.n.01': 0.2, 'radiator_cap.n.01': 0.0, 'affine.n.01': 0.1, 'number.n.01': 1.0, 'man.n.10': 1.0, 'wiggliness.n.01': 0.0, 'principle.n.01': 0.7, 'striving.n.01': 0.0, 'share.n.01': 1.0, 'promontory.n.01': 0.0, 'picture.n.01': 1.0, 'reading.n.03': 1.0, 'sanctimoniousness.n.01': 0.0, 'chalcedony.n.01': 0.0, 'stopcock.n.01': 0.0, 'backpacker.n.01': 0.1}\n"
     ]
    }
   ],
   "source": [
    "AGREEMENT_SCORES, AGREEMENT_SCORES_DICT, isHard_list, isHard_dict, timeDiffs_list, timeDiffs_dict, timeDiffs_dict_norm = compute_agreement_mean(FOLDER_PATH)\n",
    "print(AGREEMENT_SCORES_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['war.n.01', 'fiefdom.n.01', 'bed.n.03', 'return_on_invested_capital.n.01', 'texture.n.02', 'news.n.01', 'look.n.02', 'caddy.n.01', 'weeder.n.01', 'avenue.n.02', 'adar.n.01', 'bedtime.n.01', 'inversion.n.08', 'yak.n.01', 'breath.n.05', 'executive_clemency.n.01', 'muse.n.02', 'effect.n.06', 'quickening.n.02', 'sleeper.n.09', 'caravanning.n.01', 'jotter.n.01', 'armageddon.n.02', 'compass_point.n.01', 'blackwater_fever.n.01', 'respect.n.03', 'position.n.06', 'message.n.02', 'arrest.n.02', 'motivation.n.01', 'day.n.04', 'nose_cone.n.01', 'discussion.n.02', 'glow.n.05', 'alcalde.n.01', 'draft_board.n.01', 'multitude.n.03', 'hour.n.02', 'book.n.02', 'degree.n.02', 'show-stopper.n.01', 'military_position.n.01', 'top.n.09', 'pillar_of_islam.n.01', 'power.n.05', 'dail_eireann.n.01', 'record.n.05', 'pace.n.03', 'american_labor_party.n.01', 'nook_and_cranny.n.01', \"lady's_smock.n.01\", 'obstacle_race.n.01', 'clasp.n.02', 'stuff.n.07', 'succubus.n.01', 'form.n.03', 'quantity.n.02', 'commercial_enterprise.n.02', 'distaff.n.01', 'company.n.01', 'hammerlock.n.01', 'fact.n.01', 'niche.n.01', 'pinnacle.n.03', 'function.n.03', 'male_child.n.01', 'direction.n.06', 'violence.n.01', 'shape.n.01', 'movie.n.01', 'impudence.n.01', 'movement.n.04', 'argument.n.05', 'esteem.n.01', 'viewing_audience.n.01', 'incipiency.n.01', 'fire.n.02', 'handle.n.01', 'course.n.01', 'latest.n.01', 'means.n.01', 'rate.n.01', 'monetary_value.n.01', 'dwelling.n.01', 'test.n.05', 'peroration.n.02', 'light.n.01', 'accompaniment.n.02', 'fall.n.06', 'rainbow.n.02', 'stationary_stochastic_process.n.01', 'assessment.n.03', 'decision.n.01', 'homo_sapiens.n.01', 'management.n.01', 'touch.n.10', 'pommel.n.01', 'blood_brother.n.02', 'homo.n.02', 'say.n.01', 'descant.n.01', 'increase.n.03', 'board.n.01', 'rescript.n.01', 'reaction.n.03', 'exploitation.n.01', 'weather.n.01', 'schutzstaffel.n.01', 'antilogarithm.n.01', 'helpmate.n.01', 'bill.n.03', 'hegemon.n.01', 'word.n.01', 'real_world.n.01', 'car.n.01', 'sign.n.01', 'handwriting.n.01', 'hit.n.03', 'boucle.n.01', 'doggedness.n.01', 'attempt.n.01', 'town.n.01', 'aid.n.02', 'conservatory.n.02', 'art.n.03', 'toilet.n.01', 'year.n.01', 'process.n.05', 'hope.n.02', 'line.n.29', 'base.n.08', 'speech.n.02', 'basis.n.02', 'lineage.n.01', 'afternoon.n.01', 'colonialist.n.01', 'poverty_trap.n.01', 'thinking.n.01', 'affray.n.02', 'demonstrative_pronoun.n.01', \"ladies'_room.n.01\", 'burg.n.01', 'higher_law.n.01', 'doubleton.n.01', 'woman.n.01', 'fight.n.02', 'playing_field.n.02', 'road_rage.n.01', 'cusp.n.01', 'guess.n.02', 'businessmen.n.01', 'strait.n.01', 'narrow.n.01', 'left.n.02', 'sound.n.01', 'military_action.n.01', 'second_nature.n.01', 'friend.n.01', 'branch.n.03', 'outtake.n.01', 'edition.n.03', 'card.n.08', 'gazette.n.01', 'penny_ante.n.01', 'double_blind.n.01', 'mainstream.n.01', 'shooting.n.01', 'cantle.n.01', 'countenance.n.03', 'strand.n.01', 'stature.n.01', 'meaning.n.02', 'glissade.n.01', 'footrace.n.01', 'deal.n.01', 'room.n.01', 'school.n.02', 'sum.n.01', 'circumstance.n.03', 'stripe.n.04', 'link.n.06', 'predetermination.n.02', 'kin.n.01', 'relative.n.01', 'mafia.n.01', 'child.n.01', 'eastern_church.n.02', 'chine.n.02', 'pop_quiz.n.01', 'shiva.n.01', 'bonding.n.02', 'artwork.n.01', 'start.n.01', 'probability.n.01', 'meat_and_potatoes.n.01', 'part.n.09', 'myringotomy.n.01', 'south_sea_islands.n.01', 'golden_age.n.01', 'broadcast_journalist.n.01', 'issue.n.02', 'universe.n.01', 'anagnost.n.01', 'strange_attractor.n.01', 'token.n.02', 'supporter.n.01', 'plot_element.n.01', 'syndicate.n.01', 'death.n.04', 'contingent.n.02', 'thought.n.03', 'jihad.n.01', 'royal_road.n.01', 'theme.n.03', 'oblateness.n.01', 'estimate.n.01', 'nation.n.02', 'sphere.n.01', 'advertence.n.01', 'holy_order.n.01', 'judgment.n.01', 'air.n.03', 'party.n.01', 'running.n.03', 'week.n.01', 'front.n.04', 'discovery.n.01', 'counterbombardment.n.01', 'business.n.01', 'meeting.n.03', 'world_power.n.01', 'side.n.04', 'vitamin_pill.n.01', 'shadow_cabinet.n.01', 'incandescence.n.02', 'undivided_interest.n.01', 'enfilade.n.01', 'support_level.n.01', 'newspaper.n.01', 'fresco.n.01', 'logjam.n.01', 'peeper.n.02', 'trinity.n.02', 'plea_bargain.n.01', 'urgency.n.01', 'writing.n.03', 'tempestuousness.n.01', 'daub.n.01', 'sawhorse.n.01', 'greece.n.02', 'system.n.02', 'slush.n.01', 'amount.n.02', 'art_nouveau.n.01', 'fun.n.02', 'nature_study.n.01', 'act.n.04', 'auspices.n.01', 'ability.n.02', 'hose.n.03', 'opinion.n.04', 'safety_match.n.01', 'point.n.01', 'rule.n.11', 'committee.n.01', 'founder.n.02', 'eye-beaming.n.01', 'sprachgefuhl.n.01', 'unitization.n.04', 'fabric.n.01', 'silver_certificate.n.01', 'luminosity.n.01', 'wall.n.01', 'dynamism.n.03', 'earning_per_share.n.01', 'subsistence.n.01', 'convention.n.02', 'obiter_dictum.n.02', 'experience.n.02', 'hosanna.n.01', 'deposition.n.01', 'looseness.n.05', 'end.n.11', 'money.n.01', 'administrator.n.01', 'model_t.n.01', 'procedure.n.01', 'margin.n.05', 'infantry.n.01', 'store.n.02', 'particular.n.01', 'price.n.02', 'paratroops.n.01', 'sanguinity.n.01', 'meritocracy.n.01', 'conjunction.n.05', 'education.n.01', 'bilge.n.02', 'chancellor.n.03', 'music.n.01', 'meter.n.03', 'associateship.n.01', 'behalf.n.01', 'protocol.n.03', 'cry.n.01', 'painting.n.01', 'couple.n.04', 'flower.n.03', 'designation.n.03', 'cutaneous_sensation.n.01', 'support.n.06', 'fork.n.03', 'motion.n.03', 'thing.n.04', 'ground.n.05', 'athenaeum.n.01', 'juggernaut.n.01', 'office.n.04', 'work_force.n.01', 'exemplar.n.01', 'holy_year.n.01', 'numeral.n.01', 'brownie.n.01', 'shrillness.n.01', 'mire.n.01', 'hypothesis.n.02', 'arboriculture.n.01', 'sea-duty.n.01', 'peak.n.04', 'police.n.01', 'heading.n.01', 'earth.n.02', 'thelarche.n.01', 'feeling.n.06', 'source.n.03', 'manipulation.n.01', 'toleration.n.02', 'puppetry.n.01', 'condition.n.01', 'water.n.01', 'scene.n.04', 'time.n.03', 'human_body.n.01', 'country.n.02', 'palestra.n.01', 'promise.n.02', 'quality.n.03', 'report.n.01', 'hermitage.n.01', 'title.n.01', 'street.n.01', 'value.n.01', 'table.n.01', 'plot.n.01', 'eye.n.01', 'field.n.01', 'examination.n.02', 'mind_game.n.02', 'step.n.03', 'habit.n.02', 'activity.n.01', 'thousand.n.01', 'brass_monkey.n.01', 'use.n.01', 'lighter.n.02', 'head.n.04', 'soubrette.n.02', 'kind.n.01', 'justification.n.01', 'early-morning_hour.n.01', 'calendar.n.03', 'calligraphy.n.01', 'change.n.01', 'magazine.n.03', 'spinal_column.n.01', 'need.n.01', 'face.n.01', 'reputation.n.03', 'activeness.n.02', 'determination.n.02', 'spaghetti_western.n.01', 'natural_process.n.01', 'qing.n.01', 'art_class.n.01', 'military_unit.n.01', 'ruse.n.01', 'colonizer.n.01', 'bell_gable.n.01', 'place.n.10', 'voice.n.01', 'jocosity.n.01', 'interest.n.03', 'land.n.02', 'calendar_month.n.01', 'cross_section.n.03', 'union_shop.n.01', 'hairy_tongue.n.01', 'libidinal_energy.n.01', 'answer.n.01', 'velleity.n.01', 'club.n.02', 'whole.n.02', 'spirit.n.04', 'attention.n.01', 'swiss.n.01', 'region.n.01', 'godhead.n.01', 'can.n.01', 'assay.n.03', 'space.n.07', 'judge.n.01', 'crest.n.05', 'contribution.n.01', 'secondo.n.01', 'contractor.n.02', 'wish.n.01', 'ally.n.02', 'pinpoint.n.01', 'workshop.n.02', 'side_yard.n.01', 'grave.n.01', 'southwest_by_west.n.01', 'multitudinousness.n.01', 'church.n.01', 'post.n.01', 'detail.n.01', 'workplace.n.01', 'bridge_player.n.01', 'kernel.n.03', 'white.n.11', 'illustration.n.01', 'vibration.n.04', 'anapest.n.01', 'opportunity.n.01', 'meteortropism.n.01', 'rest.n.02', 'bridgehead.n.02', 'island.n.01', \"ship's_company.n.01\", 'dance_step.n.01', 'writer.n.01', 'promptness.n.01', 'domain.n.02', 'buddy.n.01', 'care.n.01', 'instruction.n.04', 'continental_shelf.n.01', 'character.n.04', 'hearing.n.05', 'back.n.08', 'trip.n.06', 'recording_studio.n.01', 'person.n.02', 'koinonia.n.01', 'undertone.n.02', 'principal.n.02', 'conclusion.n.08', 'action.n.02', 'hired_hand.n.01', 'viewgraph.n.01', 'play.n.06', 'house.n.05', 'mull.n.01', 'prodigy.n.03', 'beginning.n.02', 'breeze.n.01', 'rubicon.n.02', 'set.n.05', 'oeuvre.n.01', 'snap.n.02', 'cocktail_lounge.n.01', 'autolysis.n.01', 'body.n.02', 'terror.n.03', 'moment.n.01', 'diminution.n.02', 'trestle.n.02', 'situation.n.02', 'subsidization.n.01', 'provision.n.04', 'lateral_thinking.n.01', 'intrigue.n.01', 'talk.n.01', 'way.n.06', 'gibson_girl.n.01', 'family.n.08', 'millenary.n.03', 'desensitization_technique.n.01', 'natural_order.n.01', 'radiator_cap.n.01', 'affine.n.01', 'number.n.01', 'man.n.10', 'wiggliness.n.01', 'principle.n.01', 'striving.n.01', 'share.n.01', 'promontory.n.01', 'picture.n.01', 'reading.n.03', 'sanctimoniousness.n.01', 'chalcedony.n.01', 'stopcock.n.01', 'backpacker.n.01']\n"
     ]
    }
   ],
   "source": [
    "SYNSET_WITH_AGREEMENT_LIST = list(AGREEMENT_SCORES_DICT.keys())\n",
    "print(SYNSET_WITH_AGREEMENT_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo agreement dei synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synsets_agreement():\n",
    "    agreement_dict = {}\n",
    "    \n",
    "    for pos in SUPPORTED_POS:\n",
    "        for synset in wordnet.all_synsets(pos):\n",
    "            \n",
    "            try:\n",
    "                if synset in SYNSET_DA_EVITARE or synset.pos() == 's':\n",
    "                    continue\n",
    "\n",
    "                synset_name = synset.name()\n",
    "                if synset_name in SYNSET_WITH_AGREEMENT_LIST:\n",
    "                    agreement_score = AGREEMENT_SCORES_DICT[synset_name]\n",
    "                    ishard = isHard_dict[synset_name]\n",
    "                    timeDiffs = timeDiffs_dict[synset_name]\n",
    "                    timeDiffs_norm = timeDiffs_dict_norm[synset_name]\n",
    "                else:\n",
    "                    agreement_score = None\n",
    "                    ishard = None\n",
    "                    timeDiffs = None\n",
    "                    timeDiffs_norm = None\n",
    "                \n",
    "                agreement_dict[synset] = agreement_score, ishard, timeDiffs, timeDiffs_norm\n",
    "                \n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "    return agreement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_dict = compute_synsets_agreement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = {\n",
    "    'Synset': [str(key) for key in agreement_dict.keys()],\n",
    "    'Agreement score': [item[0] for item in agreement_dict.values()],\n",
    "    'isHard mean': [item[1] for item in agreement_dict.values()],\n",
    "    'timeDiffs mean': [item[2] for item in agreement_dict.values()],\n",
    "    'Normalized timeDiffs mean': [item[3] for item in agreement_dict.values()]\n",
    "}\n",
    "df_agreement = pd.DataFrame(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement di un synset specifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement score: 0.0\n",
      "isHard mean: 0.0\n",
      "timeDiffs mean: 2.8927\n",
      "timeDiffs mean normalized: 0.0020679478168270198\n"
     ]
    }
   ],
   "source": [
    "synset = wordnet.synset('fiefdom.n.01')\n",
    "synset_name = synset.name()\n",
    "\n",
    "if synset_name in SYNSET_WITH_AGREEMENT_LIST:\n",
    "    synset_agreement_dict = agreement_dict.get(synset, 'Synset non classificato')\n",
    "    agreement = synset_agreement_dict[0]\n",
    "    isHard_mean = synset_agreement_dict[1]\n",
    "    timeDiffs_mean = synset_agreement_dict[2]\n",
    "    timeDiffs_mean_norm = synset_agreement_dict[3]\n",
    "    print(f'Agreement score: {agreement}')\n",
    "    print(f'isHard mean: {isHard_mean}')\n",
    "    print(f'timeDiffs mean: {timeDiffs_mean}')\n",
    "    print(f'timeDiffs mean normalized: {timeDiffs_mean_norm}')\n",
    "else:\n",
    "    print('Synset senza agreement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio su file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = \"../../features/df/\"\n",
    "df_file_output = path_output + 'df_agreement.csv'\n",
    "df_agreement.to_csv(df_file_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risultati ottenuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Synset  Agreement score  isHard mean   \n",
      "0           Synset('entity.n.01')              NaN          NaN  \\\n",
      "1  Synset('physical_entity.n.01')              NaN          NaN   \n",
      "2      Synset('abstraction.n.06')              NaN          NaN   \n",
      "3            Synset('thing.n.12')              NaN          NaN   \n",
      "4           Synset('object.n.01')              NaN          NaN   \n",
      "5            Synset('whole.n.02')              1.0          0.0   \n",
      "6         Synset('congener.n.03')              NaN          NaN   \n",
      "7     Synset('living_thing.n.01')              NaN          NaN   \n",
      "8         Synset('organism.n.01')              NaN          NaN   \n",
      "9          Synset('benthos.n.02')              NaN          NaN   \n",
      "\n",
      "   timeDiffs mean  Normalized timeDiffs mean  \n",
      "0             NaN                        NaN  \n",
      "1             NaN                        NaN  \n",
      "2             NaN                        NaN  \n",
      "3             NaN                        NaN  \n",
      "4             NaN                        NaN  \n",
      "5          1.2526                   0.000895  \n",
      "6             NaN                        NaN  \n",
      "7             NaN                        NaN  \n",
      "8             NaN                        NaN  \n",
      "9             NaN                        NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_agreement[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
